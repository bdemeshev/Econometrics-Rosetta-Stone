## R

Построим простую линейную регрессию в R и проведем несложные тесты. 

Загрузим необходимые пакеты.

```{r "library chunk", results='hide', message=FALSE, warning=FALSE}

library(tidyverse) # манипуляции с данными и построение графиков
library(sjPlot) # красивые графики для линейных моделей
library(skimr) # симпатичное summary
library(rio) # чтение .dta файлов
library(car) # проверка линейных гипотез
library(tseries) # тест Харке-Бера
```

Импортируем данные.

```{r "import data", message=FALSE, warning=FALSE}
df = import("data/us-return.dta")
```

Исследуем наш датасет.

```{r "skim",  message=FALSE, warning=FALSE}
skim(df) # добавить select
```

Переименуем столбцы.

```{r "rename chunk",  message=FALSE, warning=FALSE}
df = rename(df, n = A, date = B) 
```

И уберем строчки, в которых хотя бы один элемент пустой.

```{r "omit missings",  message=FALSE, warning=FALSE}
df = na.omit(df)
```

Будем верить в CAPM :) (в начале объяснить, что такое capm)
Оценим параметры модели для компании MOTOR. Тогда зависимая переменная - разница доходностей акций MOTOR и безрискового актива, а регрессор - рыночная премия.

```{r "mutate", warning=FALSE}
df = mutate(df, y = MOTOR - RKFREE, x = MARKET - RKFREE) # придумать говорящие названия
```

Оценим нашу модель и проверим гипотезу об адекватности регрессии.

```{r "model", message=FALSE, warning=FALSE}
ols = lm(y ~ x, data = df)
summary(ols)
```

Вызовом одной функции получаем кучу полезных графиков :) 
Можем визуально оценить наличие гетероскедастичности, нормальность распределения остатков, наличие выбросов.

```{r "plot", message=FALSE, warning=FALSE}
plot(ols) # добавить, что значит каждый график, после 
```

Строим 90%-й доверительный интервал для параметров модели.

```{r "ci", warning=FALSE}
confint(ols, level = 0.9)
```

Проверим гипотезу о равенстве коэффициента при регрессоре единице.

```{r "lin hyp"}
linearHypothesis(ols, c("x = 1"))
```

Посмотрим на остатки :) Протестируем остатки регрессии на нормальность с помощью теста Харке-Бера.

\[
H_{0}: S = 0, K = 3,
\]
где S — коэффициент асимметрии (Skewness), K — коэффициент эксцесса (Kurtosis)

```{r}
jarque.bera.test(resid(ols)) 
```

И заодно посмотрим на результаты теста Шапиро – Уилка.

\[
H_{0}: \epsilon_{i} \sim  N(\mu,\sigma^2)
\]
```{r}
shapiro.test(resid(ols))
```

Оба теста указывают на нормальность распределения остатков регрессии.

Сделаем прогноз модели по 20 новым наблюдениям. Будем считать, что новые наблюдения нормально распределены с мат ожиданием и дисперсией

```{r "prediction"}
set.seed(7)

new_data = tibble(x = rnorm(10, mean = 1, sd = 3)) 
yhat = predict(ols, newdata = new_data, se = TRUE)
yhat
```