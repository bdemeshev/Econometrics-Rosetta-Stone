## R

Построим простую линейную регрессию в R и проведем несложные тесты. 

Загрузим необходимые пакеты.

```{r "library chunk", results='hide', message=FALSE, warning=FALSE}

library(tidyverse) # манипуляции с данными и построение графиков
library(sjPlot) # красивые графики для линейных моделей
library(skimr) # симпатичное summary
library(rio) # чтение .dta файлов
library(car) # проверка линейных гипотез
library(tseries) # тест Харке – Бера
```

Импортируем данные.

```{r "import data", message=FALSE, warning=FALSE}
returns = import("../data/02_us_return.dta")
```

Исследуем наш датасет.
Функция `skim()` позволяет получить красивую и приятную в работе табличку (`tibble`), содержащую типы переменных и различные описательные статистики. 
Для удобства выведем результаты только для переменных `MOTOR` и `rkfree`.

```{r "skim",  message=FALSE, warning=FALSE}
skim(returns) %>% 
  filter(skim_variable %in% c('MOTOR', 'rkfree')) 
```

Переименуем столбцы с «неговорящими» названиями :)

```{r "rename chunk",  message=FALSE, warning=FALSE}
returns = rename(returns, n = A, date = B) 
```

И уберем строчки, в которых хотя бы один элемент пустой.

```{r "omit missings",  message=FALSE, warning=FALSE}
returns= na.omit(returns)
```

Будем верить в CAPM :) (в начале объяснить, что такое capm)
Оценим параметры модели для компании MOTOR. Тогда зависимая переменная - разница доходностей акций MOTOR и безрискового актива (`motor_premium`), а регрессор - рыночная премия (`market_premium`).

```{r "mutate", warning=FALSE}
returns = mutate(returns, motor_premium = MOTOR - RKFREE, market_premium = MARKET - RKFREE)
```

Оценим нашу модель и проверим гипотезу об адекватности регрессии.

```{r "model", message=FALSE, warning=FALSE}
ols = lm(motor_premium ~ market_premium, data = returns)
summary(ols)
```

Вызовом одной функции получаем кучу полезных графиков :) 
Можем визуально оценить наличие гетероскедастичности, нормальность распределения остатков, наличие выбросов.
Без дополнительных указаний функция построит 4 графика – по одному друг за другом. 
Мы для красоты c помощью функции `par` будем выводить по два графика :)

```{r "plot", message=FALSE, warning=FALSE}
par(mfrow = c(2,2))
plot(ols) 
```

График **«Residuals vs Fitted»** помогает уловить возможные нелинейные зависимости между регрессором и объясняемой переменной. 
В «хорошем» случае мы ждем картинку с остатками, равномерно рассеянными вдоль горизонтальной прямой.
В нашем случае несколько выбиваются наблюдения с отрицательной рыночной премией.

График **«Normal Q – Q»** позволяет визуально оценить нормальность распределения остатков.
Эталоном здесь является пунктирная прямая.
На графике функция распределения остатков нашей модели «похожа» на нормальную :)

График **«Scale – Location»** дает возможность «на глаз» оценить равную дисперсию остатков регресии и проверить наличие гетероскедастичности. 
За исключением немногочисленных наблюдений с отрицательной рыночной премией, в нашем случае предположение о гомоскедастичности (одинаковой дисперсии) остатков кажется верным.

График **«Residuals vs Leverage»** помогает выявить «влиятельные наблюдения» с высоким «воздействием» (high leverage). 
Это такие наблюдения, которые имеют нетипичные для выборки значения, но исключение которых может значительно повлиять на оценки коэффициентов модели. 
На графике они располагаются справа, за границами, обозначенными красной пунктирной линией (расстояние Кука).

Теперь построим $90\%$-й доверительный интервал для параметров модели.

```{r "ci", warning=FALSE}
confint(ols, level = 0.9)
```

И заодно проверим гипотезу о равенстве коэффициента при регрессоре единице.

```{r "lin hyp"}
linearHypothesis(ols, c("market_premium = 1"))
```

Видим, что на любом разумном уровне значимости она не отвергается.

Теперь посмотрим на остатки регрессии :) Протестируем их на нормальность с помощью теста Харке – Бера.

\[
H_{0}: S = 0, K = 3,
\]
где S — коэффициент асимметрии (Skewness), K — коэффициент эксцесса (Kurtosis)

```{r}
jarque.bera.test(resid(ols)) 
```

И заодно посмотрим на результаты теста Шапиро – Уилка.

\[
H_{0}: \epsilon_{i} \sim  N(\mu,\sigma^2)
\]

```{r}
shapiro.test(resid(ols))
```

Оба теста указывают на нормальность распределения остатков.

Получим предсказания модели для обучаемой выборки.

#### ДОДУМАТЬ

```{r "fittedvalues"}
fitval = fitted(ols)
fit_vs_real = tibble(fitted = fitval, real = returns$market_premium)
```

```{r "fit vs real"}

qplot(x = 1:length(fitval), y = returns$market_premium, 
      xlab = "Number of observation", 
      ylab = "Value") + geom_point(aes(y = fitval, color = 'r'))
```

Выведем прогноз модели по $20$ новым наблюдениям (которые сами же и придумаем). 
Будем считать, что новые наблюдения распределены нормально с математическим ожиданием $0.01$ и дисперсией $0.01$.

```{r "prediction"}
set.seed(7)
new_data = tibble(market_premium = rnorm(10, mean = 0.01, sd = 0.1)) 
yhat = predict(ols, newdata = new_data, se = TRUE)
yhat$fit
```