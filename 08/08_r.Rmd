## R

Традиционно начнём в **r**.

```{r, include=FALSE}
Sys.setlocale("LC_TIME", "C")
```

Загрузим необходимые пакеты:

```{r,result=FALSE, warning=FALSE, message=FALSE}
library(xts) # работа с временными рядами
library(dplyr) # манипуляции с данными
library(ggplot2) # построение графиков
library(aTSA) # тест Дики-Фуллера
library(forecast) # прогнозирование ARMA-моделей
library(quantmod) # импортирование набора данных из интернета
library(lmtest) # проверка гипотез
```

Импортируем dataset `AAPL` прямо из пакета `quantmod`. Будем анализировать одномерный временной ряд от переменной `AAPL. Close`.

```{r, result=FALSE, warning=FALSE, message=FALSE}
getSymbols("AAPL",from = "2015-01-01",to = "2015-12-31")
```

Обозначим исследуемый набор данных как `apple_df`. 

```{r, warning=FALSE, message=FALSE}
apple_df = AAPL$AAPL.Close
```

Визуализируем изучаемый временной ряд, его автокорреляционную и частную автокорреляционную функции.

```{r, warning=FALSE}
ggtsdisplay(apple_df)
```

По графику видим, что процесс напоминает случайное блуждание: медленно убывает автокорреляционная функция, первый лаг частной автокорреляционной функции не входит в доверительный интервал, остальные - входят.

Проверим стационарность ряда тестом Дики - Фуллера.

```{r}
adf.test(apple_df)
```

Тест выявил нестационарность на 5% уровне значимости (основная гипотеза – о нестационарности ряда).

Возьмём первую разность от ряда, чтобы сделать его стационарным (ведь только стационарные процессы могут быть описаны моделью `ARMA (p, q)` ) и снова построим автокорреляционную и частную автокорреляционную функции.

```{r, warning=FALSE}
apple_diff = diff(apple_df)
ggtsdisplay(apple_diff)
summary(apple_diff)
```

Ряд похож на стационарный. 
Теперь построим несколько моделей, которые потенциально могут описать данный ряд, хотя уже заранее ожидается, что ряд в разностях будет описан `ARIMA (0, 0, 0)`, что равносильно `ARMA(0, 0)`, но всё же...

`ARIMA (0, 0, 0)`:

```{r}
arima_000 = arima(apple_diff, order = c(0, 0, 0))
summary(arima_000)
```

Построим также модель `ARIMA (1, 0, 0)` , что равносильно `ARMA (1, 0)`, для сравнения.

```{r}
arima_100 = arima(apple_diff, order = c(1, 0, 0))
summary(arima_100)
coeftest(arima_100)
```

По информационному критерию Акаике первая модель лучше (AIC меньше), а также во второй модели коэффициент перед ar(1) незначим. 

Получается, что (как и ожидалось) первая модель лучше.
Можно схитрить и использовать функцию автоподбора коэффициентов модели ARIMA.

```{r}
arima_auto_model = auto.arima(apple_diff)
summary(arima_auto_model)
```

Такая функция автоматически минимизирует критерий Акаике. 
Заметим, что автоподбор выдал модель `ARIMA (0, 0, 0)` для первой разности.

Теперь проверим остатки модели `ARIMA (0, 0, 0)` на белошумность. 
Сохраним остатки и проделаем тест Льюнг - Бокса, в котором основная гипотеза - остатки независимы. 

Сохраним остатки модели `ARIMA (0, 0, 0)` и построим тест Льюнг - Бокса (если наблюдений мало, то используем опцию `Box-Pierce`).

```{r}
res_arima_000 = resid(arima_000)
Box.test(res_arima_000, lag = 10, type = "Ljung-Box")
```

Основная гипотеза об отсутствии автокорреляции остатков отвергается, следовательно, модель корректно описывает структуру автокорреляции.

Время небольших фактов: Льюнг - это женщина-статистик!
Поэтому правильно склонять "Льюнг - Бокса", а не "Льюнга - Бокса"!

<center>
![](images/Greta.jpg)

</center>

Можно ещё также научиться оценивать визуально, где лежат корни AR и MA (`unit root test`). 
Так как для построенной модели нет AR и MA частей (`ARIMA (0, 0, 0)`), то можно применить команду к, например, `ARIMA (1, 0, 0)`:

```{r}
autoplot(arima_100)
```

Построим прогноз на 3 периода вперёд для модели `arima_000`. 
Визуализируем прогноз, границы 80% и 95% доверительного интервалов.

```{r, warning=FALSE, message=FALSE}
forecast(arima_000, h = 10) %>%
autoplot()
```
