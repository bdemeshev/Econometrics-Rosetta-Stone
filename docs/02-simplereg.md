# Коан о простой линейной регрессии {#simplereg}







## R

Построим простую линейную регрессию в R и проведем несложные тесты. 

Загрузим необходимые пакеты.


```r
library(tidyverse) # манипуляции с данными и построение графиков
library(sjPlot) # красивые графики для линейных моделей
library(skimr) # симпатичное summary
library(rio) # чтение .dta файлов
library(car) # проверка линейных гипотез
library(tseries) # тест Харке – Бера
```

Импортируем данные.


```r
returns = import("../data/02_us_return.dta")
```

Исследуем наш датасет.
Функция `skim()` позволяет получить красивую и приятную в работе табличку (`tibble`), содержащую типы переменных и различные описательные статистики. 
Для удобства выведем результаты только для переменных `MOTOR` и `rkfree`.


```r
skim(returns) %>% 
  filter(skim_variable %in% c('MOTOR', 'rkfree')) 
```


Table: (\#tab:skim)Data summary

                                   
-------------------------  --------
Name                       returns 
Number of rows             2664    
Number of columns          22      
_______________________            
Column type frequency:             
numeric                    2       
________________________           
Group variables            None    
-------------------------  --------


**Variable type: numeric**

skim_variable    n_missing   complete_rate   mean    sd      p0     p25    p50    p75   p100  hist  
--------------  ----------  --------------  -----  ----  ------  ------  -----  -----  -----  ------
MOTOR                 2544            0.05   0.02   0.1   -0.33   -0.05   0.02   0.08   0.27  ▁▂▇▇▁ 
rkfree                2544            0.05   0.01   0.0    0.00    0.01   0.01   0.01   0.01  ▂▇▇▂▂ 

Переименуем столбцы с «неговорящими» названиями :)


```r
returns = rename(returns, n = A, date = B) 
```

И уберем строчки, в которых хотя бы один элемент пустой.


```r
returns= na.omit(returns)
```

Будем верить в CAPM :) (в начале объяснить, что такое capm)
Оценим параметры модели для компании MOTOR. Тогда зависимая переменная - разница доходностей акций MOTOR и безрискового актива (`motor_premium`), а регрессор - рыночная премия (`market_premium`).


```r
returns = mutate(returns, motor_premium = MOTOR - RKFREE, market_premium = MARKET - RKFREE)
```

Оценим нашу модель и проверим гипотезу об адекватности регрессии.


```r
ols = lm(motor_premium ~ market_premium, data = returns)
summary(ols)
```

```

Call:
lm(formula = motor_premium ~ market_premium, data = returns)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.168421 -0.059381 -0.003399  0.061373  0.182991 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)    0.005253   0.007200   0.730    0.467    
market_premium 0.848150   0.104814   8.092 5.91e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.07844 on 118 degrees of freedom
Multiple R-squared:  0.3569,	Adjusted R-squared:  0.3514 
F-statistic: 65.48 on 1 and 118 DF,  p-value: 5.913e-13
```

Вызовом одной функции получаем кучу полезных графиков :) 
Можем визуально оценить наличие гетероскедастичности, нормальность распределения остатков, наличие выбросов.
Без дополнительных указаний функция построит 4 графика – по одному друг за другом. 
Мы для красоты c помощью функции `par` будем выводить по два графика :)


```r
par(mfrow = c(2,2))
plot(ols) 
```

![](02-simplereg_files/figure-epub3/plot-1.png)<!-- -->

График **«Residuals vs Fitted»** помогает уловить возможные нелинейные зависимости между регрессором и объясняемой переменной. 
В «хорошем» случае мы ждем картинку с остатками, равномерно рассеянными вдоль горизонтальной прямой.
В нашем случае несколько выбиваются наблюдения с отрицательной рыночной премией.

График **«Normal Q – Q»** позволяет визуально оценить нормальность распределения остатков.
Эталоном здесь является пунктирная прямая.
На графике функция распределения остатков нашей модели «похожа» на нормальную :)

График **«Scale – Location»** дает возможность «на глаз» оценить равную дисперсию остатков регресии и проверить наличие гетероскедастичности. 
За исключением немногочисленных наблюдений с отрицательной рыночной премией, в нашем случае предположение о гомоскедастичности (одинаковой дисперсии) остатков кажется верным.

График **«Residuals vs Leverage»** помогает выявить «влиятельные наблюдения» с высоким «воздействием» (high leverage). 
Это такие наблюдения, которые имеют нетипичные для выборки значения, но исключение которых может значительно повлиять на оценки коэффициентов модели. 
На графике они располагаются справа, за границами, обозначенными красной пунктирной линией (расстояние Кука).

Теперь построим $90\%$-й доверительный интервал для параметров модели.


```r
confint(ols, level = 0.9)
```

```
                        5 %       95 %
(Intercept)    -0.006683687 0.01718942
market_premium  0.674382048 1.02191711
```

И заодно проверим гипотезу о равенстве коэффициента при регрессоре единице.


```r
linearHypothesis(ols, c("market_premium = 1"))
```

```
Linear hypothesis test

Hypothesis:
market_premium = 1

Model 1: restricted model
Model 2: motor_premium ~ market_premium

  Res.Df     RSS Df Sum of Sq      F Pr(>F)
1    119 0.73900                           
2    118 0.72608  1  0.012915 2.0989 0.1501
```

Видим, что на любом разумном уровне значимости она не отвергается.

Теперь посмотрим на остатки регрессии :) Протестируем их на нормальность с помощью теста Харке – Бера.

\[
H_{0}: S = 0, K = 3,
\]
где S — коэффициент асимметрии (Skewness), K — коэффициент эксцесса (Kurtosis)


```r
jarque.bera.test(resid(ols)) 
```

```

	Jarque Bera Test

data:  resid(ols)
X-squared = 1.7803, df = 2, p-value = 0.4106
```

И заодно посмотрим на результаты теста Шапиро – Уилка.

\[
H_{0}: \epsilon_{i} \sim  N(\mu,\sigma^2)
\]


```r
shapiro.test(resid(ols))
```

```

	Shapiro-Wilk normality test

data:  resid(ols)
W = 0.99021, p-value = 0.5531
```

Оба теста указывают на нормальность распределения остатков.

Получим предсказания модели для обучаемой выборки.

#### ДОДУМАТЬ


```r
fitval = fitted(ols)
fit_vs_real = tibble(fitted = fitval, real = returns$market_premium)
```


```r
qplot(x = 1:length(fitval), y = returns$market_premium, 
      xlab = "Number of observation", 
      ylab = "Value") + geom_point(aes(y = fitval, color = 'r'))
```

![](02-simplereg_files/figure-epub3/fit vs real-1.png)<!-- -->

Выведем прогноз модели по $20$ новым наблюдениям (которые сами же и придумаем). 
Будем считать, что новые наблюдения распределены нормально с математическим ожиданием $0.01$ и дисперсией $0.01$.


```r
set.seed(7)
new_data = tibble(market_premium = rnorm(10, mean = 0.01, sd = 0.1)) 
yhat = predict(ols, newdata = new_data, se = TRUE)
yhat$fit
```

```
           1            2            3            4            5 
 0.207727133 -0.087769779 -0.045152029 -0.021234248 -0.068593258 
           6            7            8            9           10 
-0.066609148  0.077187768  0.003814809  0.026682011  0.199477263 
```




