[
["index.html", "Розеттский камень Коан 1 Приветственный коан 1.1 Язык программирования R 1.2 Язык программирования Python 1.3 Программа STATA", " Розеттский камень Пуассон, фея и три мексиканских негодяя 2020-01-18 Коан 1 Приветственный коан В этом коане мы рассмотрим установку и настройку R, Python и Stata. pre.r { background-color: #FEF9E7 !important; } pre.stata { background-color: #BDBDBD !important; } pre.python { background-color: #FDF2E9 !important; } 1.1 Язык программирования R R — это открытая среда программирования, помогающая в работе со статистическими данными. Для программирования на R подойдет программа RStudio. Рассмотрим установку RStudio на Mac OS и Windows. 1.1.1 Инструкция по установке RStudio для Windows / Mac OS: Загрузите и установите язык программирования R с официального сайта. Версия для Windows: Выберите “Download R for Windows” ▶ “base” ▶ “Download R 3.x.x for Windows”. Версия для Mac OS: Выберите “Download R for (Mac) OS X” ▶ “Latest Release” ▶ “R 3.x.x”. Загрузите программу RStudio с официального сайта разработчика (выберите подходящую версию из предложенных опций). Возможностей бесплатной версии будет вполне достаточно для работы. Страница загрузки 1.1.2 Начало работы Интерфейс программы New file - Создание нового файла. New project - Создание нового проекта. Open file - Открытие существующего файла. Console - Консоль, в которой набирается код. Files - Список файлов, доступных для работы. Packages - Список установленных пакетов, т.е. расширений. Также можно ознакомиться с ним, введя в консоль команду installed.packages(). Viewer - Отображение введенного кода. 1.1.3 Настройка программы1 Запустите RStudio. В разделе Tools — Global Options — Sweave — “Weave .Rnw files using” выберите knitr. Перед началом работы рекомендуется также установить ряд пакетов, то есть расширений, которые помогут при работе с данными. Для этого необходимо ввести в командную строку следующую команду: install.packages(c(&quot;vcd&quot;, &quot;ggplot2&quot;, &quot;knitr&quot;, &quot;xtable&quot;, &quot;texreg&quot;, &quot;lmtest&quot;, &quot;sandwich&quot;, &quot;erer&quot;, &quot;dplyr&quot;, &quot;readxl&quot;, &quot;reshape2&quot;, &quot;RCurl&quot;, &quot;RSelenium&quot;,&quot;XML&quot;, &quot;jsonlite&quot;, &quot;quantmod&quot;, &quot;lubridate&quot;, &quot;stringr&quot;, &quot;tidyr&quot;)) После выполнения команды все рекомендованные для использования пакеты установятся автоматически. При работе на Windows для установки пакетов может потребоваться запуск Rstudio от имени администратора (для этого необходимо кликнуть правой кнопкой мыши, выбрать “Запуск от имени администратора”). Установили? Отлично. Всё готово для использования RStudio на вашем компьютере. cat(&quot;Рабочая папка:&quot;, getwd(), &quot;\\n&quot;) Рабочая папка: /home/boris/Documents/Econometrics-Rosetta-Stone/01 1.2 Язык программирования Python Python — это открытая среда программирования, помогающая в работе со статистическими данными. Для программирования на Python подойдет программа Jupyter Notebook. 1.2.1 Установка Загрузите и установите Anaconda с официального сайта. После загрузки и установки откройте Anaconda Navigator, через который Вы сможете открыть программу Jupyter Notebook. Интерфейс Anaconda Navigator 1.2.2 Начало работы Открыв Jupyter Notebook, вы попадете на страницу, содержащую ваши сохраненные файлы. Чтобы создать новый файл, нажмите “New” ▶ “Notebook: Python 3”. Новый файл Затем, в открывшемся окне, появится новый файл. Теперь все готово к работе. Вы можете вводить свой код и затем, используя комбинацию клавиш &lt;Shift&gt; + &lt;Enter&gt;, проверять его исполнение. Ввод кода 1.3 Программа STATA Stata, в отличие от R и Python, является программой, а не языком программирования. Она также помогает в работе со статистическими данными. 1.3.1 Установка: Для установки Stata необходимо загрузить актуальную версию с сайта компании-разработчика. Подойдут как Stata SE, так и Stata MP. 1.3.2 Начало работы: Интерфейс Stata Open File - открыть файл. Save - сохранить файл. Data Editor - редактирование данных. Data Browser - просмотр данных. Variables - список переменных. Command - командная строка, в которой вводится код. Источник: https://github.com/bdemeshev/em301/wiki/R↩ "],
["simplereg.html", "Коан 2 Коан о простой линейной регрессии 2.1 R", " Коан 2 Коан о простой линейной регрессии 2.1 R Построим простую линейную регрессию в R и проведем несложные тесты. Загрузим необходимые пакеты. library(tidyverse) # манипуляции с данными и построение графиков library(sjPlot) # красивые графики для линейных моделей library(skimr) # симпатичное summary library(rio) # чтение .dta файлов library(car) # проверка линейных гипотез library(tseries) # тест Харке – Бера Импортируем данные. returns = import(&quot;../data/02_us_return.dta&quot;) Исследуем наш датасет. Функция skim() позволяет получить красивую и приятную в работе табличку (tibble), содержащую типы переменных и различные описательные статистики. Для удобства выведем результаты только для переменных MOTOR и rkfree. skim(returns) %&gt;% filter(skim_variable %in% c(&#39;MOTOR&#39;, &#39;rkfree&#39;)) Table 2.1: Data summary Name returns Number of rows 2664 Number of columns 22 _______________________ Column type frequency: numeric 2 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist MOTOR 2544 0.05 0.02 0.1 -0.33 -0.05 0.02 0.08 0.27 ▁▂▇▇▁ rkfree 2544 0.05 0.01 0.0 0.00 0.01 0.01 0.01 0.01 ▂▇▇▂▂ Переименуем столбцы с «неговорящими» названиями :) returns = rename(returns, n = A, date = B) И уберем строчки, в которых хотя бы один элемент пустой. returns= na.omit(returns) Будем верить в CAPM :) (в начале объяснить, что такое capm) Оценим параметры модели для компании MOTOR. Тогда зависимая переменная - разница доходностей акций MOTOR и безрискового актива (motor_premium), а регрессор - рыночная премия (market_premium). returns = mutate(returns, motor_premium = MOTOR - RKFREE, market_premium = MARKET - RKFREE) Оценим нашу модель и проверим гипотезу об адекватности регрессии. ols = lm(motor_premium ~ market_premium, data = returns) summary(ols) Call: lm(formula = motor_premium ~ market_premium, data = returns) Residuals: Min 1Q Median 3Q Max -0.168421 -0.059381 -0.003399 0.061373 0.182991 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.005253 0.007200 0.730 0.467 market_premium 0.848150 0.104814 8.092 5.91e-13 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.07844 on 118 degrees of freedom Multiple R-squared: 0.3569, Adjusted R-squared: 0.3514 F-statistic: 65.48 on 1 and 118 DF, p-value: 5.913e-13 Вызовом одной функции получаем кучу полезных графиков :) Можем визуально оценить наличие гетероскедастичности, нормальность распределения остатков, наличие выбросов. Без дополнительных указаний функция построит 4 графика – по одному друг за другом. Мы для красоты c помощью функции par будем выводить по два графика :) par(mfrow = c(2,2)) plot(ols) График «Residuals vs Fitted» помогает уловить возможные нелинейные зависимости между регрессором и объясняемой переменной. В «хорошем» случае мы ждем картинку с остатками, равномерно рассеянными вдоль горизонтальной прямой. В нашем случае несколько выбиваются наблюдения с отрицательной рыночной премией. График «Normal Q – Q» позволяет визуально оценить нормальность распределения остатков. Эталоном здесь является пунктирная прямая. На графике функция распределения остатков нашей модели «похожа» на нормальную :) График «Scale – Location» дает возможность «на глаз» оценить равную дисперсию остатков регресии и проверить наличие гетероскедастичности. За исключением немногочисленных наблюдений с отрицательной рыночной премией, в нашем случае предположение о гомоскедастичности (одинаковой дисперсии) остатков кажется верным. График «Residuals vs Leverage» помогает выявить «влиятельные наблюдения» с высоким «воздействием» (high leverage). Это такие наблюдения, которые имеют нетипичные для выборки значения, но исключение которых может значительно повлиять на оценки коэффициентов модели. На графике они располагаются справа, за границами, обозначенными красной пунктирной линией (расстояние Кука). Теперь построим \\(90\\%\\)-й доверительный интервал для параметров модели. confint(ols, level = 0.9) 5 % 95 % (Intercept) -0.006683687 0.01718942 market_premium 0.674382048 1.02191711 И заодно проверим гипотезу о равенстве коэффициента при регрессоре единице. linearHypothesis(ols, c(&quot;market_premium = 1&quot;)) Linear hypothesis test Hypothesis: market_premium = 1 Model 1: restricted model Model 2: motor_premium ~ market_premium Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 119 0.73900 2 118 0.72608 1 0.012915 2.0989 0.1501 Видим, что на любом разумном уровне значимости она не отвергается. Теперь посмотрим на остатки регрессии :) Протестируем их на нормальность с помощью теста Харке – Бера. \\[ H_{0}: S = 0, K = 3, \\] где S — коэффициент асимметрии (Skewness), K — коэффициент эксцесса (Kurtosis) jarque.bera.test(resid(ols)) Jarque Bera Test data: resid(ols) X-squared = 1.7803, df = 2, p-value = 0.4106 И заодно посмотрим на результаты теста Шапиро – Уилка. \\[ H_{0}: \\epsilon_{i} \\sim N(\\mu,\\sigma^2) \\] shapiro.test(resid(ols)) Shapiro-Wilk normality test data: resid(ols) W = 0.99021, p-value = 0.5531 Оба теста указывают на нормальность распределения остатков. Получим предсказания модели для обучаемой выборки. 2.1.0.1 ДОДУМАТЬ fitval = fitted(ols) fit_vs_real = tibble(fitted = fitval, real = returns$market_premium) qplot(x = 1:length(fitval), y = returns$market_premium, xlab = &quot;Number of observation&quot;, ylab = &quot;Value&quot;) + geom_point(aes(y = fitval, color = &#39;r&#39;)) Выведем прогноз модели по \\(20\\) новым наблюдениям (которые сами же и придумаем). Будем считать, что новые наблюдения распределены нормально с математическим ожиданием \\(0.01\\) и дисперсией \\(0.01\\). set.seed(7) new_data = tibble(market_premium = rnorm(10, mean = 0.01, sd = 0.1)) yhat = predict(ols, newdata = new_data, se = TRUE) yhat$fit 1 2 3 4 5 0.207727133 -0.087769779 -0.045152029 -0.021234248 -0.068593258 6 7 8 9 10 -0.066609148 0.077187768 0.003814809 0.026682011 0.199477263 "],
["binchoice.html", "Коан 3 Модель бинарного выбора 3.1 R", " Коан 3 Модель бинарного выбора Мини-теория: Линейная вероятностная модель Можно оценить вероятность бинарной зависимой переменной принимать определённое значение (чаще, 1). Линейная вероятностная модель имеет вид: \\[ P(y_i = 1) = x_i^T \\cdot \\beta + \\varepsilon_i \\] Однако такой подход обладает существенными недостатками: нереалистичное значение оцененной вероятности, ошибки, распределённые не нормально и гетероскедастичность, поэтому есть необходимость оценивания логит- и пробит- моделей. Логит - модель Предполагается, что существует скрытая (латентная) переменная, для которой строится модель, \\[ y^*_i = x_i^T \\cdot \\beta + \\varepsilon_i, \\] так, что: \\[ Y_i = \\begin{cases} 1, &amp;\\text{если ${y_i}^* \\geqslant 0$}\\\\ 0, &amp;\\text{если ${y_i}^* &lt; 0$} \\end{cases} \\] \\[ \\varepsilon_i \\sim logistic, \\\\f(t) = \\frac{e^{-t}}{(1 + e^{-t})^2} \\] LR-тест В текущем коане будем тестировать \\(H_0: \\beta_{white} = 0\\) против \\(H_a: \\beta_{white} \\neq 0\\). Статистика LR-теста имеет вид: \\[ 2 \\cdot (\\ln(L) - \\ln(L_{H_0})) \\sim \\chi^2_r, \\] где \\(\\ln(L)\\) — логарифм функции правдоподобия, \\(\\ln(L_{H_0})\\) — логарифм функции правдоподобия со значениями параметров из основной гипотезы, r - количество ограничений в основной гипотезе. Пробит-модель Также предполагается, что существует скрытая (латентная) переменная, для которой строится модель, \\[ y^*_i = x_i^T \\cdot \\beta + \\varepsilon_i, \\] так, что: \\[ Y_i = \\begin{cases} 1, &amp;\\text{если ${y_i}^* \\geqslant 0$}\\\\ 0, &amp;\\text{если ${y_i}^* &lt; 0$} \\end{cases} \\] \\[ \\varepsilon_i \\sim N(0; 1), \\\\ f(z) = \\frac{1}{\\sqrt{2 \\pi}} \\cdot \\int_{- \\infty}^{z} e^{- \\frac{t^2}{2}} dt \\] Сейчас попробуем подружиться с моделями бинарного выбора на основе данных bwght.dta, где зависимая переменная отражает, является индивид курильщиком или нет, а в качестве независимых переменных представлены характеристики индивида: количество выкуриваемых сигарет, семейный доход, налог на сигареты, цена сигарет, образование отца и матери, паритет, цвет кожи. 3.1 R Загрузим необходимы пакеты. library(rio) # импорт и экспорт данных в разных форматах library(tidyverse) # графики и манипуляции с данными library(skimr) # описательные статистики library(mfx) # нахождение предельных эффектов library(margins) # визуализация предельных эффектов library(lmtest) # проведение тестов library(plotROC) # построение ROC-кривой library(caret) # confusion-матрица library(texreg) # вывод результатов регрессии в тех и html Импортируем исследуемые данные. data = import(&quot;../data/03_bwght.dta&quot;) Рассмотрим описательные статистики по всем переменным: количество выкуриваемых сигарет, семейный доход, налог на сигареты, цена сигарет, образование отца и матери, паритет, цвет кожи. skim(data) Table 3.1: Data summary Name data Number of rows 1388 Number of columns 14 _______________________ Column type frequency: numeric 14 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist faminc 0 1.00 29.03 18.74 0.50 14.50 27.50 37.50 65.00 ▆▇▇▃▅ cigtax 0 1.00 19.55 7.80 2.00 15.00 20.00 26.00 38.00 ▂▆▇▇▁ cigprice 0 1.00 130.56 10.24 103.80 122.80 130.80 137.00 152.50 ▂▆▇▇▂ bwght 0 1.00 118.70 20.35 23.00 107.00 120.00 132.00 271.00 ▁▇▆▁▁ fatheduc 196 0.86 13.19 2.75 1.00 12.00 12.00 16.00 18.00 ▁▁▂▇▅ motheduc 1 1.00 12.94 2.38 2.00 12.00 12.00 14.00 18.00 ▁▁▂▇▃ parity 0 1.00 1.63 0.89 1.00 1.00 1.00 2.00 6.00 ▇▁▁▁▁ male 0 1.00 0.52 0.50 0.00 0.00 1.00 1.00 1.00 ▇▁▁▁▇ white 0 1.00 0.78 0.41 0.00 1.00 1.00 1.00 1.00 ▂▁▁▁▇ cigs 0 1.00 2.09 5.97 0.00 0.00 0.00 0.00 50.00 ▇▁▁▁▁ lbwght 0 1.00 4.76 0.19 3.14 4.67 4.79 4.88 5.60 ▁▁▂▇▁ bwghtlbs 0 1.00 7.42 1.27 1.44 6.69 7.50 8.25 16.94 ▁▇▆▁▁ packs 0 1.00 0.10 0.30 0.00 0.00 0.00 0.00 2.50 ▇▁▁▁▁ lfaminc 0 1.00 3.07 0.92 -0.69 2.67 3.31 3.62 4.17 ▁▁▂▆▇ Заметим существование пропущенных переменных у fatheduc, motheduc. Будем анализировать только те значения, у которых нет пропущенных наблюдений. Для этого создадим новый dataframe, data_2, в котором отсутствуют пропущенные значения. Посмотрим на его описательные статистики. data_2 = filter(data, !is.na(fatheduc), !is.na(motheduc)) skim(data_2) Table 3.2: Data summary Name data_2 Number of rows 1191 Number of columns 14 _______________________ Column type frequency: numeric 14 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist faminc 0 1 32.22 17.96 0.50 18.50 27.50 42.50 65.00 ▃▇▇▃▅ cigtax 0 1 19.60 7.86 2.00 15.00 20.00 26.00 38.00 ▂▆▇▇▂ cigprice 0 1 130.71 10.35 103.80 122.80 130.80 137.00 152.50 ▂▆▇▇▃ bwght 0 1 119.53 20.14 23.00 108.00 120.00 132.00 271.00 ▁▇▇▁▁ fatheduc 0 1 13.19 2.74 1.00 12.00 12.00 16.00 18.00 ▁▁▂▇▅ motheduc 0 1 13.13 2.42 2.00 12.00 12.00 15.00 18.00 ▁▁▂▇▃ parity 0 1 1.61 0.87 1.00 1.00 1.00 2.00 6.00 ▇▁▁▁▁ male 0 1 0.52 0.50 0.00 0.00 1.00 1.00 1.00 ▇▁▁▁▇ white 0 1 0.84 0.36 0.00 1.00 1.00 1.00 1.00 ▂▁▁▁▇ cigs 0 1 1.77 5.34 0.00 0.00 0.00 0.00 40.00 ▇▁▁▁▁ lbwght 0 1 4.77 0.19 3.14 4.68 4.79 4.88 5.60 ▁▁▂▇▁ bwghtlbs 0 1 7.47 1.26 1.44 6.75 7.50 8.25 16.94 ▁▇▇▁▁ packs 0 1 0.09 0.27 0.00 0.00 0.00 0.00 2.00 ▇▁▁▁▁ lfaminc 0 1 3.28 0.72 -0.69 2.92 3.31 3.75 4.17 ▁▁▁▅▇ Сгенерируем переменную smoke, отражающую состояние отдельного индивида: smoke = 1, если индивид курит (то есть количество выкуриваемых им сигарет положительно), smoke = 0 – если индивид не курит. data_2 = mutate(data_2, smoke = (cigs &gt; 0)) Построим модель линейной вероятности. Сохраним результат под lin_prob_model. lin_prob_model = lm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data = data_2) summary(lin_prob_model) Call: lm(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data = data_2) Residuals: Min 1Q Median 3Q Max -0.46295 -0.17696 -0.11495 -0.02127 1.01628 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.4297071 0.2270444 1.893 0.0587 . faminc -0.0014813 0.0006325 -2.342 0.0193 * cigtax 0.0008334 0.0026320 0.317 0.7516 cigprice 0.0007472 0.0019954 0.374 0.7081 fatheduc -0.0064880 0.0047493 -1.366 0.1722 motheduc -0.0242416 0.0053373 -4.542 6.14e-06 *** parity 0.0019565 0.0110725 0.177 0.8598 white 0.0471603 0.0273790 1.723 0.0852 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.3318 on 1183 degrees of freedom Multiple R-squared: 0.06448, Adjusted R-squared: 0.05895 F-statistic: 11.65 on 7 and 1183 DF, p-value: 2.184e-14 Посмотрим на число совпадений прогнозных и исходных значений. Для этого оценим предсказанные значения модели линейной вероятности. Сохраним их как переменную predictions_lin_prob_model. Если прогнозное значение выше 0.5 (порог отсечения, по дефолту, равный 0.5), то классифицируем наблюдаемого индивида как курильщика (smoke_ols = 1). Посмотрим на число совпадений исходных и прогнозных данных. predictions_lin_prob_model = predict(lin_prob_model) Генерируем smoke_ols как 1, если вероятность по модели больше 0.5 и 0, если она меньше 0.5. smoke_ols = 1 * (predictions_lin_prob_model &gt; 0.5) Число совпадений данных и прогноза модели линейной вероятности: sum(smoke_ols == data_2$smoke) [1] 1030 Известно, что модель линейной вероятности обладает значительными недостатками, поэтому оценим P(smoke=1|x), и построим логит– и пробит– модели. Построим логит-модель и сохраним результат оцененной модели как logit_model. logit_model = glm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, x = TRUE, data = data_2, family = binomial(link = &quot;logit&quot;)) summary(logit_model) Call: glm(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, family = binomial(link = &quot;logit&quot;), data = data_2, x = TRUE) Deviance Residuals: Min 1Q Median 3Q Max -1.5699 -0.5878 -0.4379 -0.2854 2.6434 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.960628 2.083625 0.461 0.64477 faminc -0.017142 0.006401 -2.678 0.00741 ** cigtax 0.013859 0.024435 0.567 0.57058 cigprice 0.004156 0.018280 0.227 0.82014 fatheduc -0.054616 0.041813 -1.306 0.19148 motheduc -0.224467 0.049228 -4.560 5.12e-06 *** parity -0.008435 0.097275 -0.087 0.93090 white 0.436632 0.260283 1.678 0.09344 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 943.55 on 1190 degrees of freedom Residual deviance: 862.11 on 1183 degrees of freedom AIC: 878.11 Number of Fisher Scoring iterations: 5 Так как коэффициенты логит- и пробит- моделей плохо интерпретируются, поскольку единицы измерения латентной переменной определить сложно, посчитаем предельные эффекты, то есть изменение вероятности решения курить с изменением фактора на 1 единицу. Для предельного эффекта в средних значениях факторов: logitmfx(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data = data_2, atmean = TRUE) Call: logitmfx(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data = data_2, atmean = TRUE) Marginal Effects: dF/dx Std. Err. z P&gt;|z| faminc -0.00168111 0.00061396 -2.7382 0.006178 ** cigtax 0.00135920 0.00239324 0.5679 0.570081 cigprice 0.00040759 0.00179294 0.2273 0.820165 fatheduc -0.00535620 0.00409569 -1.3078 0.190953 motheduc -0.02201350 0.00469099 -4.6927 2.696e-06 *** parity -0.00082727 0.00953824 -0.0867 0.930885 white 0.03815415 0.02011210 1.8971 0.057818 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 dF/dx is for discrete change for the following variables: [1] &quot;white&quot; margins = margins(logit_model) plot(margins) Интерпретация предельных эффектов (на примере переменной семейного дохода): при увеличении семейного дохода в среднем на 1 единицу при остальных неизменных факторах, вероятность стать курильщиком уменьшается в среднем на 0.18%. Визуализируем предельный эффект для семейного дохода: cplot(logit_model, &quot;faminc&quot;, what = &quot;effect&quot;, main = &quot;Average Marginal Effect of Faminc&quot;) Для определения качества модели построим классификационную матрицу. Для этого сначала вычислим предсказания логит-модели, predictions_logit_model. Так как результат не бинарный, то введём порог отсечения, равный 0.5. Назовём бинарный результат smoke_logit: predictions_logit_model = predict(logit_model) smoke_logit_model = (predictions_logit_model &gt; 0.5) Построим классификационную матрицу. При возникновении ошибок аргументов, в частности, при несовпадении их размера или типа, можно воспользоваться функцией as.factor(). confusionMatrix(as.factor(smoke_logit_model), as.factor(data_2$smoke)) Confusion Matrix and Statistics Reference Prediction FALSE TRUE FALSE 1029 161 TRUE 1 0 Accuracy : 0.864 95% CI : (0.8432, 0.883) No Information Rate : 0.8648 P-Value [Acc &gt; NIR] : 0.5546 Kappa : -0.0017 Mcnemar&#39;s Test P-Value : &lt;2e-16 Sensitivity : 0.9990 Specificity : 0.0000 Pos Pred Value : 0.8647 Neg Pred Value : 0.0000 Prevalence : 0.8648 Detection Rate : 0.8640 Detection Prevalence : 0.9992 Balanced Accuracy : 0.4995 &#39;Positive&#39; Class : FALSE Качество модели также можно проанализировать с помощью ROC-кривой, отражающей зависимость доли верных положительно классифицируемых наблюдений (sensitivity) от доли ложных положительно классифицируемых наблюдений (1-specifity). Построим ROC-кривую для логит-модели: basicplot = ggplot(data_2, aes(m = predictions_logit_model, d = data_2$smoke)) + geom_roc() basicplot + annotate(&quot;text&quot;, x = .75, y = .25, label = paste(&quot;AUC =&quot;, round(calc_auc(basicplot)$AUC, 2))) Площадь под кривой обозначается как AUC. Она показывает качество классификации. Соответственно, чем выше AUC, тем лучше построенная модель. Сейчас проанализируем правильность спецификации логит-модели. Может быть, лучше будет убрать какую-нибудь переменную? Рассмотрим логит-модель, не учитывающую переменную white. Сохраним эту модель под названием logit_model_new. logit_model_new = glm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity, x = TRUE, data = data_2, family = binomial(link = &quot;logit&quot;)) Сравним модели logit_model и logit_model_new с помощью теста максимального правдоподобия (likelihood ratio test). lrtest(logit_model,logit_model_new) Likelihood ratio test Model 1: smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white Model 2: smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity #Df LogLik Df Chisq Pr(&gt;Chisq) 1 8 -431.06 2 7 -432.55 -1 2.9988 0.08333 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 p-value = 0.08 в LR-тесте. Следовательно, основная гипотеза о том, что переменная white не влияет на решение стать курильщиком, не отвергается на 5% уровне значимости. Сейчас посмотрим на пробит-модель. Скрытая переменная в этой модели распределена стандартно нормально: \\[ f(t) = \\frac{1 \\cdot e^{\\frac{-t^2}{2}}}{\\sqrt{2 \\cdot \\pi}} \\] Построим пробит-модель. probit_model = glm(smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, data = data_2, family = binomial(link = &quot;probit&quot;)) summary(probit_model) Call: glm(formula = smoke ~ 1 + faminc + cigtax + cigprice + fatheduc + motheduc + parity + white, family = binomial(link = &quot;probit&quot;), data = data_2) Deviance Residuals: Min 1Q Median 3Q Max -1.5255 -0.5947 -0.4376 -0.2607 2.7564 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.393063 1.130957 0.348 0.72818 faminc -0.008873 0.003376 -2.628 0.00858 ** cigtax 0.005892 0.013245 0.445 0.65643 cigprice 0.003561 0.009930 0.359 0.71987 fatheduc -0.034593 0.023160 -1.494 0.13527 motheduc -0.125693 0.027090 -4.640 3.49e-06 *** parity -0.003052 0.053610 -0.057 0.95460 white 0.242348 0.140052 1.730 0.08356 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 943.55 on 1190 degrees of freedom Residual deviance: 858.93 on 1183 degrees of freedom AIC: 874.93 Number of Fisher Scoring iterations: 5 Вычисление предельных эффектов и их интерпретация, построение классификационной матрицы и ROC-кривой и LR-тест проводятся аналогично выполненным в логит-модели. Выведем сравнительную таблицу для построенных моделей. screenreg(list(lin_prob_model, logit_model, probit_model), custom.model.names = c(&quot;Модель линейной вероятности&quot;, &quot;Логит-модель&quot;, &quot;Пробит-модель&quot;)) ========================================================================== Модель линейной вероятности Логит-модель Пробит-модель -------------------------------------------------------------------------- (Intercept) 0.43 0.96 0.39 (0.23) (2.08) (1.13) faminc -0.00 * -0.02 ** -0.01 ** (0.00) (0.01) (0.00) cigtax 0.00 0.01 0.01 (0.00) (0.02) (0.01) cigprice 0.00 0.00 0.00 (0.00) (0.02) (0.01) fatheduc -0.01 -0.05 -0.03 (0.00) (0.04) (0.02) motheduc -0.02 *** -0.22 *** -0.13 *** (0.01) (0.05) (0.03) parity 0.00 -0.01 -0.00 (0.01) (0.10) (0.05) white 0.05 0.44 0.24 (0.03) (0.26) (0.14) -------------------------------------------------------------------------- R^2 0.06 Adj. R^2 0.06 Num. obs. 1191 1191 1191 RMSE 0.33 AIC 878.11 874.93 BIC 918.77 915.59 Log Likelihood -431.06 -429.46 Deviance 862.11 858.93 ========================================================================== *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 "],
["multchoice.html", "Коан 4 Модели множественного выбора 4.1 R", " Коан 4 Модели множественного выбора 4.1 R Загрузим необходимые пакеты. library(tidyverse) # манипуляции с данными и построени графиков library(skimr) # симпатичное summary library(rio) # чтения .dta файлов library(margins) # расчет предельных эффектов library(mlogit) library(nnet) Импортируем датасет. В нем находятся данные по клиентам пенсионных фондов. Нас интересует переменная pctstck, которая принимает три значения: 0, 50, 100 в зависимоcти от ответа респондента на вопрос о предпочтительном способе инвестирования пенсионных накоплений — в облигации, смешанным способом или в акции. ОЧЕНЬ ДЛИННОЕ ПРЕДЛОЖЕНИЕ!!!!!! df = import(&quot;../data/04_pension.dta&quot;) Начнем с пристального взгляда на описательные статистки. skim(df) Table 2.1: Data summary Name df Number of rows 226 Number of columns 19 _______________________ Column type frequency: numeric 19 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist id 0 1.00 2445.09 1371.27 38 1312.5 2377.50 3804.25 5014 ▇▇▇▇▅ pyears 8 0.96 11.39 9.61 0 4.0 9.00 16.00 45 ▇▅▂▁▁ prftshr 20 0.91 0.21 0.41 0 0.0 0.00 0.00 1 ▇▁▁▁▂ choice 0 1.00 0.62 0.49 0 0.0 1.00 1.00 1 ▅▁▁▁▇ female 0 1.00 0.60 0.49 0 0.0 1.00 1.00 1 ▅▁▁▁▇ married 0 1.00 0.73 0.44 0 0.0 1.00 1.00 1 ▃▁▁▁▇ age 0 1.00 60.70 4.29 53 57.0 60.00 64.00 73 ▇▇▇▂▁ educ 7 0.97 13.52 2.55 8 12.0 12.00 16.00 18 ▁▇▂▂▂ finc25 10 0.96 0.21 0.41 0 0.0 0.00 0.00 1 ▇▁▁▁▂ finc35 10 0.96 0.19 0.39 0 0.0 0.00 0.00 1 ▇▁▁▁▂ finc50 10 0.96 0.25 0.43 0 0.0 0.00 0.00 1 ▇▁▁▁▂ finc75 10 0.96 0.12 0.33 0 0.0 0.00 0.00 1 ▇▁▁▁▁ finc100 10 0.96 0.12 0.33 0 0.0 0.00 0.00 1 ▇▁▁▁▁ finc101 10 0.96 0.06 0.25 0 0.0 0.00 0.00 1 ▇▁▁▁▁ wealth89 0 1.00 197.91 242.09 -580 52.0 127.85 247.50 1485 ▁▇▂▁▁ black 0 1.00 0.12 0.33 0 0.0 0.00 0.00 1 ▇▁▁▁▁ stckin89 0 1.00 0.32 0.47 0 0.0 0.00 1.00 1 ▇▁▁▁▃ irain89 0 1.00 0.50 0.50 0 0.0 0.50 1.00 1 ▇▁▁▁▇ pctstck 0 1.00 46.68 39.44 0 0.0 50.00 100.00 100 ▇▁▇▁▆ Отсюда несложно заметить, что переменная choice — бинарная. И принимает значение 1, если индивид в выборке имел право выбора схемы инвестирования. Переменнная wealth98 — чистое богатство пенсионеров на 1989 год. Остальные переменные нас пока что не интересуют :) Для начала разберемся с объясняемой переменной. Превратим её в факторную и упорядочим категории. df = mutate(df, y = factor(pctstck), y = relevel(y, ref = 2)) levels(df$y) [1] &quot;50&quot; &quot;0&quot; &quot;100&quot; Можно взглянуть на значения объясняемой переменной в разрезе какой-то другой переменной. table(df$y, df$educ) 8 9 10 11 12 13 14 15 16 17 18 50 1 1 0 3 34 4 6 2 14 5 14 0 5 3 0 3 31 4 7 0 11 1 7 100 0 2 1 1 36 1 5 4 5 4 4 Построим модель множественного выбора (лог-линейная модель). multmodel = multinom(y ~ choice + age + educ + wealth89 + prftshr, data = df, reflevel = &#39;0&#39;) # weights: 21 (12 variable) initial value 220.821070 iter 10 value 208.003694 iter 20 value 204.508245 final value 204.507779 converged summary(multmodel) Call: multinom(formula = y ~ choice + age + educ + wealth89 + prftshr, data = df, reflevel = &quot;0&quot;) Coefficients: (Intercept) choice age educ wealth89 prftshr 0 -3.7778032 -0.626950541 0.10621896 -0.1851817 3.716695e-04 0.2718066 100 0.7155771 -0.002461399 0.01139191 -0.1387427 1.684637e-05 1.2527158 Std. Errors: (Intercept) choice age educ wealth89 prftshr 0 1.577870 0.3701450 0.02706225 0.06827224 0.0007353231 0.4980872 100 1.368662 0.3876881 0.02549556 0.06954044 0.0007948194 0.4622377 Residual Deviance: 409.0156 AIC: 433.0156 При необходимости можем построить модельку для подвыборки, например, только для замужних/женатых. multmodel_married = multinom(y ~ choice + age + educ + wealth89 + prftshr, subset = married == 1, data = df, reflevel = &#39;0&#39;) # weights: 21 (12 variable) initial value 165.890456 iter 10 value 152.140783 iter 20 value 149.611135 final value 149.611069 converged summary(multmodel_married) Call: multinom(formula = y ~ choice + age + educ + wealth89 + prftshr, data = df, subset = married == 1, reflevel = &quot;0&quot;) Coefficients: (Intercept) choice age educ wealth89 prftshr 0 -4.9076771 -1.0040922 0.1279089 -0.1905437 0.0006204087 -0.1901239 100 0.2276417 -0.5382183 0.0133528 -0.1000726 0.0004076403 1.0692773 Std. Errors: (Intercept) choice age educ wealth89 prftshr 0 1.850782 0.4463254 0.03189365 0.07925101 0.0008454956 0.5621150 100 1.603713 0.4540907 0.02965149 0.07925338 0.0008825075 0.4947089 Residual Deviance: 299.2221 AIC: 323.2221 Быстренько прикинули значимость коэффициентов. coef(multmodel)/summary(multmodel)$standard.errors (Intercept) choice age educ wealth89 prftshr 0 -2.3942416 -1.693797014 3.9249859 -2.712401 0.50545058 0.5457008 100 0.5228296 -0.006348915 0.4468195 -1.995136 0.02119521 2.7101114 Сохраним прогнозы. fit_values = fitted(multmodel) И посчитаем относительное изменение отношения шансов: \\[ \\frac{P(y_{i} = j)}{P(y_{i} = 1)} = exp(x_{i}\\beta) \\] показывает изменение отношения шансов при выборе альтернативы j вместо базовой альтернативы 1, если x изменился на единицу. odds.ratio(multmodel) Error in odds.ratio(multmodel): could not find function &quot;odds.ratio&quot; Можем посчитать предельные эффекты в различных квартилях. summary(marginal_effects(multmodel)) dydx_choice dydx_age dydx_educ Min. :0.02341 Min. :-0.019966 Min. :0.01298 1st Qu.:0.05924 1st Qu.:-0.015237 1st Qu.:0.03141 Median :0.07324 Median :-0.013644 Median :0.03791 Mean :0.06945 Mean :-0.012893 Mean :0.03468 3rd Qu.:0.08402 3rd Qu.:-0.011486 3rd Qu.:0.03905 Max. :0.11414 Max. :-0.005686 Max. :0.04223 dydx_wealth89 dydx_prftshr Min. :-6.855e-05 Min. :-0.25602 1st Qu.:-5.119e-05 1st Qu.:-0.19363 Median :-4.509e-05 Median :-0.16877 Mean :-4.275e-05 Mean :-0.15863 3rd Qu.:-3.729e-05 3rd Qu.:-0.13454 Max. :-1.675e-05 Max. :-0.03909 Или при заданном значении объясняемых переменных. margins(multmodel, at = list(age = 69, choice = 1)) at(age) at(choice) choice age educ wealth89 prftshr 69 1 0.08024 -0.01431 0.03263 -4.857e-05 -0.1158 Теперь посмотрим на модель упорядоченного выбора :) Для нее возьмем другие данные. Выборку позаимствуем из опроса NLSY (National Longitudinal Survey of Youth). В ней представлены данные о 3705 молодых белых женщинах из США. Зависимая переменная tradrole — степень согласия с утверждением «Место женщины дома, а не на работе» по четырехбалльной шкале (1 – категорически не согласна, 2 – не согласна, 3 – согласна, 4 – совершенно согласна). data_nlsy = import(&#39;data/tradrole.dta&#39;) Error in import(&quot;data/tradrole.dta&quot;): No such file # skim(data_nlsy) qplot(data_nlsy, x = tradrole) + xlab(&#39;Ответы респонденток&#39;) + ggtitle(&#39;Вот такие дела, джентельмены :)&#39;) Error in FUN(X[[i]], ...): object &#39;tradrole&#39; not found Посмотрим, как влияет религиозное воспитание (cath — католичество и fpro — протестанство), число лет образования матери — meduc и проживание в крупном городе urb на объясняемую переменную. #сначала в факторные, потом регрессия oprobit = polr(as.factor(tradrole) ~ as.factor(cath) + as.factor(fpro) + meduc + as.factor(urb), data = data_nlsy, method = &quot;probit&quot;, na.action = na.omit) Error in polr(as.factor(tradrole) ~ as.factor(cath) + as.factor(fpro) + : could not find function &quot;polr&quot; summary(oprobit) Error in summary(oprobit): object &#39;oprobit&#39; not found В summary видим коэффициенты при регрессорах и коэффициенты при константах для каждой из упорядоченных альтернатив. summary(oprobit) Error in summary(oprobit): object &#39;oprobit&#39; not found "],
["poisreg.html", "Коан 5 Модели счетных данных 5.1 R", " Коан 5 Модели счетных данных 5.1 R Загрузим необходимые пакеты. library(tidyverse) # манипуляции с данными и построение графиков library(sjPlot) # визуализация моделей library(skimr) # симпатичное summary library(rio) # чтение .dta файлов library(MASS) # отрицательное биномиальное распределение library(lmtest) # проверка гипотез library(pscl) # zero-inflation function library(margins) # для подсчета предельных эффектов Импортируем данные. df_fish = rio::import(file = &quot;../data/05_fish.dta&quot;) Данные содержат информацию о количестве рыбы, пойманной людьми на отдыхе. Camper - наличие/отсутствие палатки. Child - количество детей, которых взяли на рыбалку. Persons - количество людей в группе. Count - количество пойманной рыбы Посмотрим нам описательные статистики. skim(df_fish) Table 2.1: Data summary Name df_fish Number of rows 250 Number of columns 4 _______________________ Column type frequency: numeric 4 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist camper 0 1 0.59 0.49 0 0 1 1 1 ▆▁▁▁▇ child 0 1 0.68 0.85 0 0 0 1 3 ▇▅▁▂▁ count 0 1 3.30 11.64 0 0 0 2 149 ▇▁▁▁▁ persons 0 1 2.53 1.11 1 2 2 4 4 ▆▇▁▆▇ Переменная camper принимает всего два значения, поэтому превратим ее в факторную переменную. df_fish = mutate(df_fish, camper = factor(camper)) Наша задача - по имеющимся данным предсказать улов. Для начала посмотрим на распределение объясняемой переменной count. ggplot(df_fish, aes(x = count)) + geom_histogram(binwidth = 1) + labs(x = &#39;count&#39;, y = &#39;frequency&#39;, title = &#39;Distribution of count variable&#39;) Предположим, что переменная имеет распределение Пуассона. Будем использовать пуассоновскую регрессию. \\[ P(y=k)=exp(-\\lambda) \\lambda^k / k! \\] где \\(\\lambda=\\exp(b_1 +b_2*x)\\) poisson_model = glm(count ~ child + camper + persons, family = &quot;poisson&quot;, data = df_fish) summary(poisson_model) Call: glm(formula = count ~ child + camper + persons, family = &quot;poisson&quot;, data = df_fish) Deviance Residuals: Min 1Q Median 3Q Max -6.8096 -1.4431 -0.9060 -0.0406 16.1417 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.98183 0.15226 -13.02 &lt;2e-16 *** child -1.68996 0.08099 -20.87 &lt;2e-16 *** camper1 0.93094 0.08909 10.45 &lt;2e-16 *** persons 1.09126 0.03926 27.80 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2958.4 on 249 degrees of freedom Residual deviance: 1337.1 on 246 degrees of freedom AIC: 1682.1 Number of Fisher Scoring iterations: 6 Посчитаем средний предельный эффект для каждой переменной. m = margins(poisson_model) summary(m) factor AME SE z p lower upper camper1 2.5815 0.2137 12.0800 0.0000 2.1626 3.0003 child -5.5701 0.3300 -16.8779 0.0000 -6.2169 -4.9233 persons 3.5968 0.1801 19.9697 0.0000 3.2438 3.9498 cplot(poisson_model, x = &#39;persons&#39;, what = &#39;effect&#39;, title = &#39;Предельный эффект переменной camper&#39;) margins(poisson_model, at = list(child = 0:1)) at(child) child persons camper1 0 -12.948 8.361 6.343 1 -2.389 1.543 1.171 plot_model(poisson_model, type = &#39;pred&#39;) $child $camper $persons plot_model(poisson_model, type = &quot;pred&quot;, terms = c(&quot;child [0, 0, 1]&quot;, &quot;persons [1,3]&quot;)) Однако, заметим, что дисперсия и среднее значение объясняемой переменной не равны, как это предполагает распределение Пуассона. df_fish %&gt;% group_by(camper) %&gt;% summarize(var = var(count), mean = mean(count)) # A tibble: 2 x 3 camper var mean &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 21.1 1.52 2 1 212. 4.54 Оценим регрессию, предполагая отрицательное биномиальное распределение остатков. В этом случае, дисперсия распределения зависит от некоторого параметра и не равна среднему. nb1 = glm.nb(count ~ child + camper + persons, data = df_fish) summary(nb1) Call: glm.nb(formula = count ~ child + camper + persons, data = df_fish, init.theta = 0.4635287626, link = log) Deviance Residuals: Min 1Q Median 3Q Max -1.6673 -0.9599 -0.6590 -0.0319 4.9433 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.6250 0.3304 -4.918 8.74e-07 *** child -1.7805 0.1850 -9.623 &lt; 2e-16 *** camper1 0.6211 0.2348 2.645 0.00816 ** persons 1.0608 0.1144 9.273 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for Negative Binomial(0.4635) family taken to be 1) Null deviance: 394.25 on 249 degrees of freedom Residual deviance: 210.65 on 246 degrees of freedom AIC: 820.44 Number of Fisher Scoring iterations: 1 Theta: 0.4635 Std. Err.: 0.0712 2 x log-likelihood: -810.4440 Попробуем исключить из модели переменную camper и сравним качество двух моделей. nb2 = update(nb1, . ~ . - camper) waldtest(nb1, nb2) Wald test Model 1: count ~ child + camper + persons Model 2: count ~ child + persons Res.Df Df F Pr(&gt;F) 1 246 2 247 -1 6.9979 0.008686 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Можем посмотреть на результаты модели с “раздутыми нулями” (zero-inflated). Они предполагают большую частоту нулевых наблюдений. zero_infl = zeroinfl(count ~ child + camper | persons, data = df_fish, dist = &#39;negbin&#39;) summary(zero_infl) Call: zeroinfl(formula = count ~ child + camper | persons, data = df_fish, dist = &quot;negbin&quot;) Pearson residuals: Min 1Q Median 3Q Max -0.5861 -0.4617 -0.3886 -0.1974 18.0135 Count model coefficients (negbin with log link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.3710 0.2561 5.353 8.64e-08 *** child -1.5153 0.1956 -7.747 9.41e-15 *** camper1 0.8791 0.2693 3.265 0.0011 ** Log(theta) -0.9854 0.1760 -5.600 2.14e-08 *** Zero-inflation model coefficients (binomial with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.6031 0.8365 1.916 0.0553 . persons -1.6666 0.6793 -2.453 0.0142 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Theta = 0.3733 Number of iterations in BFGS optimization: 22 Log-likelihood: -432.9 on 6 Df plot_model(zero_infl, type = &#39;pred&#39;) $child $camper "],
["arma.html", "Коан 6 ARMA 6.1 R", " Коан 6 ARMA Достигнем просветления в анализе временных рядов вместе с нашими друзьями, Stata, R и Python! В качестве анализируемых наблюдений используем данные по стоимости акций коммапнии Apple c 2015-01-01 по 2015-12-31: цена открытия/ закрытия, минимальная/ максимальная цены, объём и скорректиованная цена. 6.1 R Традиционно начнём в r. Загрузим необходимые пакеты: library(xts) # работа с временными рядами library(dplyr) # манипуляции с данными library(ggplot2) # построение графиков library(aTSA) # тест Дики-Фуллера library(forecast) # прогнозирование ARMA-моделей library(quantmod) # импортирование набора данных из интернета library(lmtest) # проверка гипотез Импортируем dataset AAPL прямо из пакета quantmod. Будем анализировать одномерный временной ряд от переменной AAPL. Close. getSymbols(&quot;AAPL&quot;,from = &quot;2015-01-01&quot;,to = &quot;2015-12-31&quot;) [1] &quot;AAPL&quot; Обозначим исследуемый набор данных как apple_df. apple_df = AAPL$AAPL.Close Визуализируем изучаемый временной ряд, его автокорреляционную и частную автокорреляционную функции. ggtsdisplay(apple_df) По графику видим, что процесс напоминает случайное блуждание: медленно убывает автокорреляционная функция, первый лаг частной автокорреляционной функции не входит в доверительный интервал, остальные - входят. Проверим стационарность ряда тестом Дики - Фуллера. adf.test(apple_df) Augmented Dickey-Fuller Test alternative: stationary Type 1: no drift no trend lag ADF p.value [1,] 0 0.0708 0.664 [2,] 1 0.1616 0.690 [3,] 2 0.1670 0.692 [4,] 3 0.1321 0.682 [5,] 4 0.0120 0.647 Type 2: with drift no trend lag ADF p.value [1,] 0 2.15 0.99 [2,] 1 2.06 0.99 [3,] 2 2.24 0.99 [4,] 3 2.35 0.99 [5,] 4 2.65 0.99 Type 3: with drift and trend lag ADF p.value [1,] 0 1.75 0.99 [2,] 1 1.53 0.99 [3,] 2 1.64 0.99 [4,] 3 1.77 0.99 [5,] 4 2.17 0.99 ---- Note: in fact, p.value = 0.01 means p.value &lt;= 0.01 Тест выявил нестационарность на 5% уровне значимости (основная гипотеза – о нестационарности ряда). Возьмём первую разность от ряда, чтобы сделать его стационарным (ведь только стационарные процессы могут быть описаны моделью ARMA (p, q) ) и снова построим автокорреляционную и частную автокорреляционную функции. apple_diff = diff(apple_df) ggtsdisplay(apple_diff) summary(apple_diff) Index AAPL.Close Min. :2015-01-02 Min. :-6.89000 1st Qu.:2015-04-04 1st Qu.:-1.02500 Median :2015-07-02 Median :-0.07500 Mean :2015-07-02 Mean :-0.00804 3rd Qu.:2015-09-30 3rd Qu.: 1.14499 Max. :2015-12-30 Max. : 6.17000 NA&#39;s :1 Ряд похож на стационарный. Теперь построим несколько моделей, которые потенциально могут описать данный ряд, хотя уже заранее ожидается, что ряд в разностях будет описан ARIMA (0, 0, 0), что равносильно ARMA(0, 0), но всё же… ARIMA (0, 0, 0): arima_000 = arima(apple_diff, order = c(0, 0, 0)) summary(arima_000) Call: arima(x = apple_diff, order = c(0, 0, 0)) Coefficients: intercept -0.0080 s.e. 0.1244 sigma^2 estimated as 3.867: log likelihood = -523.79, aic = 1051.58 Training set error measures: ME RMSE MAE MPE MAPE MASE Training set 8.078552e-15 1.966425 1.495158 99.55996 99.55996 0.6825331 ACF1 Training set -0.02936922 Построим также модель ARIMA (1, 0, 0) , что равносильно ARMA (1, 0), для сравнения. arima_100 = arima(apple_diff, order = c(1, 0, 0)) summary(arima_100) Call: arima(x = apple_diff, order = c(1, 0, 0)) Coefficients: ar1 intercept -0.0296 -0.0075 s.e. 0.0635 0.1208 sigma^2 estimated as 3.863: log likelihood = -523.68, aic = 1053.36 Training set error measures: ME RMSE MAE MPE MAPE MASE Training set -0.0003728078 1.965566 1.491983 94.09101 105.0814 0.6810838 ACF1 Training set -0.002372191 coeftest(arima_100) z test of coefficients: Estimate Std. Error z value Pr(&gt;|z|) ar1 -0.0296313 0.0634712 -0.4668 0.6406 intercept -0.0075101 0.1207545 -0.0622 0.9504 По информационному критерию Акаике первая модель лучше (AIC меньше), а также во второй модели коэффициент перед ar(1) незначим. Получается, что (как и ожидалось) первая модель лучше. Можно схитрить и использовать функцию автоподбора коэффициентов модели ARIMA. arima_auto_model = auto.arima(apple_diff) summary(arima_auto_model) Series: apple_diff ARIMA(0,0,0) with zero mean sigma^2 estimated as 3.867: log likelihood=-523.79 AIC=1049.58 AICc=1049.6 BIC=1053.1 Training set error measures: ME RMSE MAE MPE MAPE MASE Training set -0.008040008 1.966441 1.49548 -37.76185 445.4945 0.6841274 ACF1 Training set -0.02936922 Такая функция автоматически минимизирует критерий Акаике. Заметим, что автоподбор выдал модель ARIMA (0, 0, 0) для первой разности. Теперь проверим остатки модели ARIMA (0, 0, 0) на белошумность. Сохраним остатки и проделаем тест Льюнг - Бокса, в котором основная гипотеза - остатки независимы. Сохраним остатки модели ARIMA (0, 0, 0) и построим тест Льюнг - Бокса (если наблюдений мало, то используем опцию Box-Pierce). res_arima_000 = resid(arima_000) Box.test(res_arima_000, lag = 10, type = &quot;Ljung-Box&quot;) Box-Ljung test data: res_arima_000 X-squared = 4.2362, df = 10, p-value = 0.9361 Основная гипотеза об отсутствии автокорреляции остатков отвергается, следовательно, модель корректно описывает структуру автокорреляции. Время небольших фактов: Льюнг - это женщина-статистик! Поэтому правильно склонять “Льюнг - Бокса”, а не “Льюнга - Бокса”! Можно ещё также научиться оценивать визуально, где лежат корни AR и MA (unit root test). Так как для построенной модели нет AR и MA частей (ARIMA (0, 0, 0)), то можно применить команду к, например, ARIMA (1, 0, 0): autoplot(arima_100) Построим прогноз на 3 периода вперёд для модели arima_000. Визуализируем прогноз, границы 80% и 95% доверительного интервалов. forecast(arima_000, h = 10) %&gt;% autoplot() "],
["paneldata.html", "Коан 7 Коан о панельных данных", " Коан 7 Коан о панельных данных "],
["heterosked.html", "Коан 8 Гетероскедастичность в простой регрессии 8.1 R", " Коан 8 Гетероскедастичность в простой регрессии Одним из нарушений условий ТГМ является гетероскедастичность, возникающая ввиду неодинаковых дисперсий для разных наблюдений. Она нежелательна ввиду того, что оценки МНК не являются эффективными (но остаются несмещёнными), и предпосылки для использования t-статистик нарушены, что даёт неверный результат о значимости коэффициентов. Этот коан благословит Вас на поиски гетероскедастичности и просветит о способах борьбы с ней. Будем анализировать гетероскедастичность на данных о стоимости квартир. Мини-теория: Тест Уайта Он неконструктивный, он может лишь показать наличие гетероскедастичности, асимптотический. Нормальность остатков в предпосылках не требуется, подразумевается, что \\[E({\\varepsilon^4_i}) = const\\]. \\[ \\begin{cases} H_0: \\sigma^2_i = \\sigma^2 \\\\ H_1: \\sigma^2_i \\neq = \\sigma^2 \\\\ \\end{cases} \\] На первом шаге тест сохраняет остатки от построения начальной регрессии. \\[ \\hat{\\ln{(pricemetr_i)}} = \\hat{\\beta}_0 + \\hat{\\beta}_{\\ln{(kitsp)}} \\cdot \\ln{(kitsp_i)} + \\hat{\\beta}_{\\ln{(livesp)}}\\cdot \\ln{(livesp_i)} + \\hat{\\beta}_{\\ln{(dist)}}\\cdot \\ln{(dist_i)} + \\hat{\\beta}_{\\ln{(metrdist)}}\\cdot \\ln{(metrdist_i)} \\] На втором - строится вспомогательная регрессия (X_j-вектор j-го фактора). \\[ \\hat{e}^2_i = \\hat{\\alpha}_0 + \\sum_{j=1}^{k} \\hat{\\alpha}_j \\cdot X_j + \\sum_{j=1}^{k} \\hat{\\gamma}_j \\cdot X^2_j + \\sum_{j &lt; m}^{k} \\hat{\\delta}_j X_j \\cdot X_m \\] R-squared построенной вспомогательной регрессии должен быть распределён как: \\[ n \\cdot R^2_{aux} \\sim \\chi^2_{K-1} \\] где K – число факторов во вспомогательной регрессии. Тест Бройша - Пагана Тест Бройша - Пагана — обобщённый вариант теста Уайта. В тесте Бройша-Пагана во вспомогательной регрессии можно брать любые функции от регрессоров, в тесте Уайта - регрессоры, их квадраты и кросс-произведения. Тест Бройша-Пагана является асимптотическим. \\[ \\begin{cases} H_0: \\sigma^2_i = \\sigma^2 \\\\ H_1: \\sigma^2_i \\propto f(\\alpha_0 + \\alpha_1 \\cdot Z_1 + \\ldots + \\alpha_p \\cdot Z_p) \\\\ \\end{cases} \\] Классическая версия Бройша - Пагана строится на основе метода максимального правдоподобия. Предпосылками классической версии теста являются нормальность остатков, существование у функции дисперсии из альтернативной гипотезы первой и второй производной. Считается LM-статистика, которая, при верной основной гипотезе об отсутствии гетероскедастичности, имеет хи-квадратное распределение с p-1 степенью свободы. Современная модификация теста не требует нормальности остатков, лишь \\[{\\mathbb E}({\\varepsilon^4_i}) = const\\]. На первом шаге строится исходная регрессия и сохраняются остатки. Затем строится состоятельная оценка дисперсии: \\[ \\hat{\\sigma}^2 = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} {e^2_i} \\] Потом строится вспомогательная регрессия: \\[ \\frac{e^2}{\\hat{\\sigma}^2} = \\alpha_0 + \\alpha_1 \\cdot Z_1 + \\ldots + \\alpha_p \\cdot Z_p + u \\] И рассчитывается тестовая статистика: \\[ \\frac{RSS_{aux}}{2} \\sim \\chi^2_{p} \\] Тест Голдфелда - Квандта \\[ \\begin{cases} H_0: \\sigma^2_i = \\sigma^2 \\\\ H_1: \\sigma^2_i \\propto X_i \\\\ \\end{cases} \\] Этот тест предполагает нормальность остатков и является неасимптотическим. Процедура: Сначала все наблюдения сортируются по возрастанию абсолютного значения фактора, вызывающего гетероскедастичность. Затем отсортированный ряд по фактору делится на 3 примерно равные части. Считаются гетероскедастичности по первой и третьей части ряда. Строится F- статистика: \\[ \\frac{RSS_2}{RSS_1} \\sim F_{r - k, r-k} \\] WLS как способ борьбы с гетероскедастичностью Веса – оценка обратной дисперсии переменной, вызывающей гетероскедачность. То есть оценим регрессию: \\[ \\frac{\\ln{(pricemetr_i)}}{\\hat{\\sigma}_i} = \\frac{\\beta_0}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(kitsp)}} \\cdot \\ln{(kitsp_i)}}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(livesp)}} \\cdot \\ln{(livesp_i)}}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(dist)}} \\cdot \\ln{(dist_i)}}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(metrdist)}} \\cdot \\ln{(metrdist_i)}}{\\hat{\\sigma}_i} + \\frac{\\varepsilon_i}{\\hat{\\sigma}_i} \\] где r - размер первой и третьей частей отсортированного ряда. 8.1 R Вызовем R в помощь в охоте на гетероскедастичность. Импортируем его оружейные пакеты. library(rio) # импорт и экспорт данных в разных форматах library(dplyr) # манипуляции с данными library(lmtest) # тест Бройша-Пагана library(sandwich) # оценка дисперсии при гетероскедастичности library(UStatBookABSC) # WLS library(estimatr) # получение робастных оценок library(ggpubr) # для графиков library(skimr) # для описательных статистик Импортируем наш dataset, flats.dta: flats = import(&quot;../data/10_flats.dta&quot;) Рассмотрим описательные статистики загруженного датасета. skim(flats) Table 3.2: Data summary Name flats Number of rows 773 Number of columns 44 _______________________ Column type frequency: character 2 numeric 42 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace sanuzel__1_razdel__0_sovm_ 0 1 1 2 0 3 0 okrug 0 1 0 4 1 4 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist n 0 1 395.48 228.89 1.00 196.00 397.00 595.00 7.880e+02 ▇▇▇▇▇ rooms 0 1 1.00 0.00 1.00 1.00 1.00 1.00 1.000e+00 ▁▁▇▁▁ price 0 1 7548674.66 2492693.71 3300000.00 5750000.00 6700000.00 8600000.00 1.750e+07 ▇▇▃▁▁ totsp 0 1 37.82 9.90 15.00 32.00 38.00 42.00 7.500e+01 ▂▇▃▂▁ livesp 0 1 21.45 7.25 8.00 18.00 19.00 21.00 6.500e+01 ▇▆▁▁▁ kitsp 0 1 8.09 2.85 2.00 6.00 8.00 10.00 2.400e+01 ▅▇▂▁▁ dist 0 1 12.19 4.66 5.00 9.00 11.50 15.00 2.550e+01 ▆▇▅▁▁ metrdist 0 1 8.83 4.44 1.00 5.00 10.00 10.00 4.000e+01 ▇▇▁▁▁ walk 0 1 0.46 0.50 0.00 0.00 0.00 1.00 1.000e+00 ▇▁▁▁▇ brick 0 1 0.16 0.37 0.00 0.00 0.00 0.00 1.000e+00 ▇▁▁▁▂ tel 0 1 0.91 0.30 0.00 1.00 1.00 1.00 3.000e+00 ▁▇▁▁▁ bal 0 1 0.53 0.50 0.00 0.00 1.00 1.00 1.000e+00 ▇▁▁▁▇ floor 0 1 0.83 0.38 0.00 1.00 1.00 1.00 1.000e+00 ▂▁▁▁▇ new 0 1 0.31 0.46 0.00 0.00 0.00 1.00 1.000e+00 ▇▁▁▁▃ floors 0 1 15.71 9.09 3.00 9.00 14.00 19.00 4.800e+01 ▇▅▂▂▁ nfloor 0 1 7.93 7.99 1.00 3.00 6.00 11.00 1.270e+02 ▇▁▁▁▁ floor1 0 1 0.10 0.30 0.00 0.00 0.00 0.00 1.000e+00 ▇▁▁▁▁ floor2 0 1 0.07 0.26 0.00 0.00 0.00 0.00 1.000e+00 ▇▁▁▁▁ nw 0 1 0.41 0.49 0.00 0.00 0.00 1.00 1.000e+00 ▇▁▁▁▆ w 0 1 0.20 0.40 0.00 0.00 0.00 0.00 1.000e+00 ▇▁▁▁▂ sw 0 1 0.39 0.49 0.00 0.00 0.00 1.00 1.000e+00 ▇▁▁▁▅ price_meter 0 1 207308.48 74861.22 84930.44 167187.50 189393.90 223076.90 6.000e+05 ▇▇▁▁▁ ln_price_meter 0 1 12.19 0.30 11.35 12.03 12.15 12.32 1.330e+01 ▁▇▇▁▁ ln_livesp 0 1 3.02 0.27 2.08 2.89 2.94 3.04 4.170e+00 ▁▃▇▁▁ ln_totsp 0 1 3.60 0.28 2.71 3.47 3.64 3.74 4.320e+00 ▁▁▇▅▁ ln_kitsp 0 1 2.02 0.41 0.69 1.79 2.08 2.30 3.180e+00 ▂▁▇▇▁ ln_dist 0 1 2.43 0.40 1.61 2.20 2.44 2.71 3.240e+00 ▂▃▇▅▂ ln_metrdist 0 1 2.04 0.55 0.00 1.61 2.30 2.30 3.690e+00 ▁▁▆▇▁ ln_metrdist_walk 0 1 0.97 1.09 0.00 0.00 0.00 2.08 3.690e+00 ▇▁▃▃▁ ln_livesp_sq 0 1 9.22 1.74 4.32 8.35 8.67 9.27 1.743e+01 ▁▇▁▁▁ ln_totsp_sq 0 1 13.01 1.94 7.33 12.01 13.23 13.97 1.864e+01 ▁▂▇▃▁ ln_kitsp_sq 0 1 4.24 1.48 0.48 3.21 4.32 5.30 1.010e+01 ▂▇▆▁▁ ln_dist_sq 0 1 6.04 1.88 2.59 4.83 5.97 7.33 1.049e+01 ▅▅▇▅▂ ln_metrdist_sq 0 1 4.49 2.07 0.00 2.59 5.30 5.30 1.361e+01 ▅▇▃▁▁ ln_floors 0 1 2.59 0.59 1.10 2.20 2.64 2.94 3.870e+00 ▃▅▆▇▃ ln_nfloor 0 1 1.70 0.89 0.00 1.10 1.79 2.40 4.840e+00 ▅▆▇▂▁ ln_floors_sq 0 1 7.05 3.03 1.21 4.83 6.96 8.67 1.499e+01 ▃▇▇▅▂ ln_nfloor_sq 0 1 3.69 3.04 0.00 1.21 3.21 5.75 2.347e+01 ▇▃▁▁▁ non_livesp 0 1 8.29 4.65 0.00 5.00 8.00 11.00 2.500e+01 ▅▇▅▁▁ ln_totsp2 0 1 13.01 1.94 7.33 12.01 13.23 13.97 1.864e+01 ▁▂▇▃▁ ln_totsp3 0 1 28.34 8.55 8.99 24.15 25.53 28.22 7.274e+01 ▂▇▁▁▁ ln_price_metr 0 1 12.19 0.30 11.35 12.03 12.15 12.32 1.330e+01 ▁▇▇▁▁ Построим простую линейную регрессионную модель, на которой будем проверять гетероскедастичность. reg = lm(ln_price_metr ~ 1 + ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data = flats) summary(reg) Call: lm(formula = ln_price_metr ~ 1 + ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data = flats) Residuals: Min 1Q Median 3Q Max -0.62723 -0.16125 -0.00845 0.13614 0.77618 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 14.19920 0.13492 105.243 &lt; 2e-16 *** ln_livesp -0.16053 0.03723 -4.312 1.83e-05 *** ln_kitsp -0.29913 0.02300 -13.007 &lt; 2e-16 *** ln_dist -0.33025 0.02367 -13.952 &lt; 2e-16 *** ln_metrdist -0.05738 0.01560 -3.679 0.000251 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.2309 on 768 degrees of freedom Multiple R-squared: 0.4179, Adjusted R-squared: 0.4149 F-statistic: 137.9 on 4 and 768 DF, p-value: &lt; 2.2e-16 Проверим наличие гетероскедастичности визуально. Построим зависимости цены квартир от объясняющих факторов. kit = ggplot(flats) + geom_point(aes(x = ln_kitsp, y = ln_price_metr)) + labs(x = &quot;Площадь кухни, кв.м&quot;, y = &quot;Цена квартиры, $1000&quot;, title = &quot;Стоимость квартир в Москве&quot;) live = ggplot(flats) + geom_point(aes(x = ln_livesp, y = ln_price_metr)) + labs(x = &quot;Жилая площадь, кв.м&quot;, y = &quot;Цена квартиры, $1000&quot;, title = &quot;Стоимость квартир в Москве&quot;) dist = ggplot(flats) + geom_point(aes(x = ln_dist, y = ln_price_metr)) + labs(x = &quot;Расстояние до центра, м&quot;, y = &quot;Цена квартиры, $1000&quot;, title = &quot;Стоимость квартир в Москве&quot;) metrdist = ggplot(flats) + geom_point(aes(x = ln_metrdist, y = ln_price_metr)) + labs(x = &quot;Расстояние до метро, м&quot;, y = &quot;Цена квартиры, $1000&quot;, title = &quot;Стоимость квартир в Москве&quot;) ggarrange(kit, live, dist, metrdist, ncol = 2, nrow = 2) Из сета красивых графиков видно, что гетероскедастичность присутствует. В частности, подозрительны переменные ln_kitsp и ln_metrdist. Проверим наличие гетероскедастичности с помощью тестов. Начнём с теста Уайта. Тест Уайта реализуется в R (ручками) как: bptest(reg, varformula = ~ (ln_livesp + ln_kitsp + ln_dist + ln_metrdist)^2 + I(ln_livesp ^ 2) + I(ln_kitsp ^ 2) + I(ln_dist ^ 2) + I(ln_metrdist ^ 2), data = flats) studentized Breusch-Pagan test data: reg BP = 89.02, df = 14, p-value = 5.81e-13 Тест Уайта выявил гетероскедастичность. Проведём тест Бройша - Пагана. Классическая версия Бройша - Пагана реализуется в r по команде: bptest(reg, studentize = FALSE) Breusch-Pagan test data: reg BP = 18.39, df = 4, p-value = 0.001035 Модифицированная версия теста Бройша - Пагана реализуется по команде: bptest(reg) studentized Breusch-Pagan test data: reg BP = 15.778, df = 4, p-value = 0.003332 Причем, если отдельно не указать спецификацию вспомогательной регрессии, то bptest() возьмёт все регрессоры исходной модели. В обеих версиях теста Бройша - Пагана гетероскедастичность обнаружена. Ещё есть тест Голдфелда - Квандта. Предположим, что дисперсии случайных ошибок растут с ростом площади кухни, kitsp. flats_ordered = arrange(flats, kitsp) reg_gqtest = lm(ln_price_metr ~ ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data = flats_ordered) gqtest(reg_gqtest, fraction = 0.34) # посередине отсортированного ряда лежит 34% наблюдений Goldfeld-Quandt test data: reg_gqtest GQ = 1.1072, df1 = 251, df2 = 250, p-value = 0.2106 alternative hypothesis: variance increases from segment 1 to 2 Будет также полезным познакомиться с методами борьбы с гетероскедастичностью. Способ 1. Взвешенный МНК. В R его можно осуществить так: reg_wls = lm(data = flats, ln_price_metr ~ ln_livesp + ln_kitsp + ln_dist + ln_metrdist, weights = fitted(lm(abs(residuals(reg)) ~ ln_kitsp)) ^ 2) summary(reg_wls) Call: lm(formula = ln_price_metr ~ ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data = flats, weights = fitted(lm(abs(residuals(reg)) ~ ln_kitsp))^2) Weighted Residuals: Min 1Q Median 3Q Max -0.105299 -0.029659 -0.001107 0.025053 0.155867 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 14.28313 0.13557 105.357 &lt; 2e-16 *** ln_livesp -0.16102 0.03849 -4.183 3.2e-05 *** ln_kitsp -0.33901 0.02245 -15.098 &lt; 2e-16 *** ln_dist -0.33075 0.02406 -13.749 &lt; 2e-16 *** ln_metrdist -0.05859 0.01587 -3.691 0.000239 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.04242 on 768 degrees of freedom Multiple R-squared: 0.4683, Adjusted R-squared: 0.4655 F-statistic: 169.1 on 4 and 768 DF, p-value: &lt; 2.2e-16 !!!! Проверить веса!!!! Способ 2. Робастные оценки Уайта. reg_hc = lm_robust(data = flats, ln_price_metr ~ ln_livesp + ln_kitsp + ln_dist + ln_metrdist) summary(reg_hc) Call: lm_robust(formula = ln_price_metr ~ ln_livesp + ln_kitsp + ln_dist + ln_metrdist, data = flats) Standard error type: HC2 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF (Intercept) 14.19920 0.15982 88.846 0.000e+00 13.88546 14.51293 768 ln_livesp -0.16053 0.04114 -3.902 1.039e-04 -0.24129 -0.07976 768 ln_kitsp -0.29913 0.02855 -10.476 4.275e-24 -0.35518 -0.24308 768 ln_dist -0.33025 0.02409 -13.710 1.992e-38 -0.37754 -0.28296 768 ln_metrdist -0.05738 0.01460 -3.930 9.271e-05 -0.08604 -0.02871 768 Multiple R-squared: 0.4179 , Adjusted R-squared: 0.4149 F-statistic: 102.1 on 4 and 768 DF, p-value: &lt; 2.2e-16 Робастные оценки коэффициентов регрессии получаются состоятельными. "]
]
