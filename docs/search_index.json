[
["index.html", "Розеттский камень Коан 1 Напутственное слово", " Розеттский камень Пуассон, фея и три мексиканских негодяя 2020-01-18 Коан 1 Напутственное слово pre.r { background-color: #FEF9E7 !important; } pre.stata { background-color: #BDBDBD !important; } pre.python { background-color: #FDF2E9 !important; } "],
["installsoft.html", "Коан 2 Коан об установке софта", " Коан 2 Коан об установке софта В этом коане мы рассмотрим установку и настройку программ для работы на языках программирования R и Python, а также установку и настройку программы Stata. "],
["simplereg.html", "Коан 3 Коан о простой линейной регрессии 3.1 Python", " Коан 3 Коан о простой линейной регрессии 3.1 Python Опять начинаем с импорта пакетов. Почти всегда джентельменский набор включает в себя pandas, numpy и matplotlib.pyplot. Много полезных функций для статистических расчетов можно найти в модуле Statsmodels. import pandas as pd # работа с таблицами import numpy as np # математические функции и матрицы import matplotlib.pyplot as plt # графики import seaborn as sns # еще более классные графики import statsmodels.api as sm # стандартные регресионные модели import statsmodels.formula.api as smf # аналогичные модели с синтаксисом в стиле R import statsmodels.graphics.gofplots as gf # визуализация моделей import statsmodels.discrete.discrete_model # дискретные модели from statsmodels.stats.outliers_influence import summary_table # работа с выбросами from scipy.stats import shapiro # тест Шапиро – Уилка При желании можем настроить графики по своему вкусу, изменив стиль и другие параметры шрифтов и графиков :) plt.style.use(&#39;seaborn&#39;) plt.rc(&#39;font&#39;, size=14) plt.rc(&#39;figure&#39;, titlesize=15) plt.rc(&#39;axes&#39;, labelsize=15) plt.rc(&#39;axes&#39;, titlesize=15) Загрузим данные. returns = pd.read_stata(&#39;../data/02_us_return.dta&#39;) Переименуем столбцы с «неговорящими» названиями :) Параметр inplace=True здесь (и во многих других функциях) позволяет изменить объект, не возвращая его копию. returns.rename(columns={&#39;A&#39;:&#39;n&#39;, &#39;B&#39;: &#39;date&#39;}, inplace=True) Избавимся от наблюдений с пропущенными значениями. returns.dropna(inplace=True) Как и прежде, рассмотрим CAPM модель :) Оценим бэту для компании MOTOR. Тогда зависимая переменная - разница доходностей акций MOTOR и безрискового актива (motor_premium), а регрессор - рыночная премия (market_premium). returns[&#39;motor_premium&#39;] = returns[&#39;MOTOR&#39;] - returns[&#39;RKFREE&#39;] returns[&#39;market_premium&#39;] = returns[&#39;MARKET&#39;] - returns[&#39;RKFREE&#39;] Оценим модель и посмотрим на саммари :) regr = smf.ols(&#39;motor_premium ~ market_premium&#39;, data=returns).fit() regr.summary() &lt;class &#39;statsmodels.iolib.summary.Summary&#39;&gt; &quot;&quot;&quot; OLS Regression Results ============================================================================== Dep. Variable: motor_premium R-squared: 0.357 Model: OLS Adj. R-squared: 0.351 Method: Least Squares F-statistic: 65.48 Date: Sat, 18 Jan 2020 Prob (F-statistic): 5.91e-13 Time: 13:07:40 Log-Likelihood: 136.18 No. Observations: 120 AIC: -268.4 Df Residuals: 118 BIC: -262.8 Df Model: 1 Covariance Type: nonrobust ================================================================================== coef std err t P&gt;|t| [0.025 0.975] ---------------------------------------------------------------------------------- Intercept 0.0053 0.007 0.730 0.467 -0.009 0.020 market_premium 0.8481 0.105 8.092 0.000 0.641 1.056 ============================================================================== Omnibus: 2.684 Durbin-Watson: 2.030 Prob(Omnibus): 0.261 Jarque-Bera (JB): 1.780 Skew: -0.031 Prob(JB): 0.411 Kurtosis: 2.406 Cond. No. 14.6 ============================================================================== Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. &quot;&quot;&quot; Можем посчитать прогнозное значение. returns[&#39;yhat&#39;] = regr.fittedvalues # в R добавить для исходных, а сюда для новых Красивые графики для остатков, выборосов и прочих радостей, как в R, придется строить ручками. Зато приятно поиграть с оформлением :) fig, ax = plt.subplots() ax.plot(returns[&#39;x&#39;],regr.fittedvalues, color=&#39;g&#39;, alpha=0.8) Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: &#39;x&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 2975, in __getitem__ indexer = self.columns.get_loc(key) File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\indexes\\base.py&quot;, line 2892, in get_loc return self._engine.get_loc(self._maybe_cast_indexer(key)) File &quot;pandas/_libs/index.pyx&quot;, line 107, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/index.pyx&quot;, line 131, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item ax.scatter(returns[&#39;x&#39;],regr.fittedvalues + regr.resid, color=&#39;g&#39;, alpha=0.8, s=40) Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: &#39;x&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 2975, in __getitem__ indexer = self.columns.get_loc(key) File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\indexes\\base.py&quot;, line 2892, in get_loc return self._engine.get_loc(self._maybe_cast_indexer(key)) File &quot;pandas/_libs/index.pyx&quot;, line 107, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/index.pyx&quot;, line 131, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item ax.vlines(returns[&#39;x&#39;],regr.fittedvalues,regr.fittedvalues + regr.resid, color=&#39;gray&#39;, alpha=0.5) Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: &#39;x&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 2975, in __getitem__ indexer = self.columns.get_loc(key) File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\indexes\\base.py&quot;, line 2892, in get_loc return self._engine.get_loc(self._maybe_cast_indexer(key)) File &quot;pandas/_libs/index.pyx&quot;, line 107, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/index.pyx&quot;, line 131, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item plt.title(&#39;Линия регрессии и остатки&#39;) plt.xlabel(&#39;RKFREE&#39;) plt.ylabel(&#39;MARKET&#39;) plt.show() Строим \\(90\\%\\)-й доверительный интервал. regr.conf_int(alpha = 0.1) 0 1 Intercept -0.006684 0.017189 market_premium 0.674382 1.021917 И проведем F-test. hypotheses = &#39;(x = 1)&#39; regr.f_test(r_matrix = hypotheses) Error in py_call_impl(callable, dots$args, dots$keywords): PatsyError: unrecognized token in constraint (x = 1) ^ Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\statsmodels\\base\\model.py&quot;, line 1648, in f_test res = self.wald_test(r_matrix, cov_p=cov_p, invcov=invcov, use_f=True) File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\statsmodels\\base\\model.py&quot;, line 1719, in wald_test LC = DesignInfo(names).linear_constraint(r_matrix) File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\patsy\\design_info.py&quot;, line 536, in linear_constraint return linear_constraint(constraint_likes, self.column_names) File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\patsy\\constraint.py&quot;, line 403, in linear_constraint tree = parse_constraint(code, variable_names) File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\patsy\\constraint.py&quot;, line 237, in parse_constraint return infix_parse(_tokenize_constraint(string, variable_names), File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\patsy\\constraint.py&quot;, line 196, in _tokenize_constraint Origin(string, offset, offset + 1)) Тест Шапиро - Уилка. Такой же, как и в R. W, p_value = shapiro(regr.resid) Генерируем новые данные и строим предсказание. import random random.seed(7) new_data = returns[&#39;x&#39;] + 0.5 * np.random.normal(len(returns)) Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: &#39;x&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 2975, in __getitem__ indexer = self.columns.get_loc(key) File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\indexes\\base.py&quot;, line 2892, in get_loc return self._engine.get_loc(self._maybe_cast_indexer(key)) File &quot;pandas/_libs/index.pyx&quot;, line 107, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/index.pyx&quot;, line 131, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item prediction = regr.predict(new_data) Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;new_data&#39; is not defined Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; А теперь жесть! Построим графички, похожие на plot() R. fig_1 = plt.figure(1) fig_1.axes[0] = sns.residplot(returns[&#39;x&#39;], returns[&#39;y&#39;], lowess=True, scatter_kws={&#39;alpha&#39;: 0.6}, line_kws={&#39;color&#39;: &#39;red&#39;, &#39;lw&#39;: 2, &#39;alpha&#39;: 0.8}) Error in py_call_impl(callable, dots$args, dots$keywords): KeyError: &#39;x&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\frame.py&quot;, line 2975, in __getitem__ indexer = self.columns.get_loc(key) File &quot;C:\\Users\\DNS\\ANACON~1\\lib\\site-packages\\pandas\\core\\indexes\\base.py&quot;, line 2892, in get_loc return self._engine.get_loc(self._maybe_cast_indexer(key)) File &quot;pandas/_libs/index.pyx&quot;, line 107, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/index.pyx&quot;, line 131, in pandas._libs.index.IndexEngine.get_loc File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item fig_1.axes[0].set_title(&#39;Residuals vs Fitted&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): IndexError: list index out of range Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_1.axes[0].set_xlabel(&#39;Fitted values&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): IndexError: list index out of range Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; fig_1.axes[0].set_ylabel(&#39;Residuals&#39;) # можем добавить метки потенциальных аутлаеров Error in py_call_impl(callable, dots$args, dots$keywords): IndexError: list index out of range Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; abs_resid = abs(regr.resid).sort_values(ascending=False) abs_resid_top3=abs_resid[:3] for i in abs_resid_top3.index: fig_1.axes[0].annotate(i, xy = (regr.fittedvalues[i], regr.resid[i])) Error in py_call_impl(callable, dots$args, dots$keywords): IndexError: list index out of range Detailed traceback: File &quot;&lt;string&gt;&quot;, line 2, in &lt;module&gt; norm_residuals = regr.get_influence().resid_studentized_internal # сохраним стьюдентизированные остатки QQ = gf.ProbPlot(norm_residuals) fig_2 = QQ.qqplot(line=&#39;45&#39;, alpha=0.5, color=&#39;b&#39;, lw=1) fig_2.axes[0].set_title(&#39;Normal Q-Q&#39;) fig_2.axes[0].set_xlabel(&#39;Theoretical Quantiles&#39;) fig_2.axes[0].set_ylabel(&#39;Standardized Residuals&#39;); #и снова метки abs_norm_resid = np.flip(np.argsort(abs(norm_residuals)), 0) abs_norm_resid_top3 = abs_norm_resid[:3] for r, i in enumerate(abs_norm_resid_top3): fig_2.axes[0].annotate(i, xy=(np.flip(QQ.theoretical_quantiles, 0)[r], norm_residuals[i])) fig_3 = plt.figure(3) plt.scatter(regr.fittedvalues, np.sqrt(abs(norm_residuals)), alpha=0.5) sns.regplot(regr.fittedvalues, np.sqrt(abs(norm_residuals)), scatter=False, ci=False, lowess=True, line_kws={&#39;color&#39;: &#39;red&#39;, &#39;lw&#39;: 1, &#39;alpha&#39;: 0.6}) fig_3.axes[0].set_title(&#39;Scale-Location&#39;) fig_3.axes[0].set_xlabel(&#39;Fitted values&#39;) fig_3.axes[0].set_ylabel(&#39;$\\sqrt{|Standardized Residuals|}$&#39;) # и еще раз!) abs_sq_norm_resid = np.flip(np.argsort(np.sqrt(abs(norm_residuals)), 0)) abs_sq_norm_resid_top3 = abs_sq_norm_resid[:3] for i in abs_sq_norm_resid_top3: fig_3.axes[0].annotate(i, xy=(regr.fittedvalues[i], np.sqrt(abs(norm_residuals)[i]))) leverage = regr.get_influence().hat_matrix_diag # сохраняем элементы матрицы-шляпницы cook_dist = regr.get_influence().cooks_distance[0] # и расстояние Кука fig_4 = plt.figure(4) plt.scatter(leverage, norm_residuals, alpha=0.5) sns.regplot(leverage, norm_residuals, scatter=False, ci=False, lowess=True, line_kws={&#39;color&#39;: &#39;red&#39;, &#39;lw&#39;: 1, &#39;alpha&#39;: 0.8}) fig_4.axes[0].set_xlim(0, 0.20) # РАЗВЕСТИ НА ОТДЕЛЬНЫЕ ЧАНКИ ИЛИ MESSAGE = FALSE (0, 0.2) fig_4.axes[0].set_ylim(-3, 5) (-3, 5) fig_4.axes[0].set_title(&#39;Residuals vs Leverage&#39;) fig_4.axes[0].set_xlabel(&#39;Leverage&#39;) fig_4.axes[0].set_ylabel(&#39;Standardized Residuals&#39;) leverage_top3 = np.flip(np.argsort(cook_dist), 0)[:3] for i in leverage_top3: fig_4.axes[0].annotate(i, xy=(leverage[i], norm_residuals[i])) plt.show() "],
["binchoice.html", "Коан 4 Модель бинарного выбора", " Коан 4 Модель бинарного выбора Мини-теория: Линейная вероятностная модель Можно оценить вероятность бинарной зависимой переменной принимать определённое значение (чаще, 1). Линейная вероятностная модель имеет вид: \\[ P(y_i = 1) = x_i^T \\cdot \\beta + \\varepsilon_i \\] Однако такой подход обладает существенными недостатками: нереалистичное значение оцененной вероятности, ошибки, распределённые не нормально и гетероскедастичность, поэтому есть необходимость оценивания логит- и пробит- моделей. Логит - модель Предполагается, что существует скрытая (латентная) переменная, для которой строится модель, \\[y^*_i = x_i^T \\cdot \\beta + \\varepsilon_i\\], так, что: \\[ \\begin{equation*} Y_i = \\begin{cases} 1, &amp;\\text{если ${y_i}^* \\geqslant 0$}\\\\ 0, &amp;\\text{если ${y_i}^* &lt; 0$} \\end{cases} \\end{equation*} \\] \\[\\varepsilon_i \\sim logistic, \\\\f(t) = \\frac{e^{-t}}{(1 + e^{-t})^2}\\] LR-тест В текущем коане будем тестировать \\[H_0: \\beta_{white} = 0\\] против \\[H_a: \\beta_{white} \\neq 0\\]. Статистика LR-теста имеет вид: \\[2 \\cdot (\\ln(L) - \\ln(L_{H_0})) \\sim \\chi^2_r\\], где \\(ln(L)\\) - логарифм функции правдоподобия, \\(ln(L_{H_0})\\) - логарифм функции правдоподобия со значениями параметров из основной гипотезы, r - количество ограничений в основной гипотезе. Пробит-модель Также предполагается, что существует скрытая (латентная) переменная, для которой строится модель, \\[y^*_i = x_i^T \\cdot \\beta + \\varepsilon_i\\], так, что: \\[ \\begin{equation*} Y_i = \\begin{cases} 1, &amp;\\text{если ${y_i}^* \\geqslant 0$}\\\\ 0, &amp;\\text{если ${y_i}^* &lt; 0$} \\end{cases} \\end{equation*} \\] \\[\\varepsilon_i \\sim N(0; 1), \\\\ f(z) = \\frac{1}{\\sqrt{2 \\pi}} \\cdot \\int_{- \\infty}^{z} e^{- \\frac{t^2}{2}} dt\\] Сейчас попробуем подружиться с моделями бинарного выбора на основе данных bwght.dta, где зависимая переменная отражает, является индивид курильщиком или нет, а в качестве независимых переменных представлены характеристики индивида: количество выкуриваемых сигарет, семейный доход, налог на сигареты, цена сигарет, образование отца и матери, паритет, цвет кожи. "],
["multchoice.html", "Коан 5 Модели множественного выбора", " Коан 5 Модели множественного выбора "],
["poisreg.html", "Коан 6 Модели счетных данных", " Коан 6 Модели счетных данных "],
["arma.html", "Коан 7 ARMA", " Коан 7 ARMA Достигнем просветления в анализе временных рядов вместе с нашими друзьями, Stata, R и Python! В качестве анализируемых наблюдений используем данные по стоимости акций коммапнии Apple c 2015-01-01 по 2015-12-31: цена открытия/ закрытия, минимальная/ максимальная цены, объём и скорректиованная цена. "],
["paneldata.html", "Коан 8 Коан о панельных данных", " Коан 8 Коан о панельных данных "],
["heterosked.html", "Коан 9 Гетероскедастичность в простой регрессии", " Коан 9 Гетероскедастичность в простой регрессии Одним из нарушений условий ТГМ является гетероскедастичность, возникающая ввиду неодинаковых дисперсий для разных наблюдений. Она нежелательна ввиду того, что оценки МНК не являются эффективными (но остаются несмещёнными), и предпосылки для использования t-статистик нарушены, что даёт неверный результат о значимости коэффициентов. Этот коан благословит Вас на поиски гетероскедастичности и просветит о способах борьбы с ней. Будем анализировать гетероскедастичность на данных о стоимости квартир. Мини-теория: Тест Уайта Он неконструктивный, он может лишь показать наличие гетероскедастичности, асимптотический. Нормальность остатков в предпосылках не требуется, подразумевается, что \\[E({\\varepsilon^4_i}) = const\\]. \\[ \\begin{cases} H_0: \\sigma^2_i = \\sigma^2 \\\\ H_1: \\sigma^2_i \\neq = \\sigma^2 \\\\ \\end{cases} \\] На первом шаге тест сохраняет остатки от построения начальной регрессии. \\[ \\hat{\\ln{(pricemetr_i)}} = \\hat{\\beta}_0 + \\hat{\\beta}_{\\ln{(kitsp)}} \\cdot \\ln{(kitsp_i)} + \\hat{\\beta}_{\\ln{(livesp)}}\\cdot \\ln{(livesp_i)} + \\hat{\\beta}_{\\ln{(dist)}}\\cdot \\ln{(dist_i)} + \\hat{\\beta}_{\\ln{(metrdist)}}\\cdot \\ln{(metrdist_i)} \\] На втором - строится вспомогательная регрессия (X_j-вектор j-го фактора). \\[ \\hat{e}^2_i = \\hat{\\alpha}_0 + \\sum_{j=1}^{k} \\hat{\\alpha}_j \\cdot X_j + \\sum_{j=1}^{k} \\hat{\\gamma}_j \\cdot X^2_j + \\sum_{j &lt; m}^{k} \\hat{\\delta}_j X_j \\cdot X_m \\] R-squared построенной вспомогательной регрессии должен быть распределён как: \\[ n \\cdot R^2_{aux} \\sim \\chi^2_{K-1} \\] где K – число факторов во вспомогательной регрессии. Тест Бройша - Пагана Тест Бройша - Пагана — обобщённый вариант теста Уайта. В тесте Бройша-Пагана во вспомогательной регрессии можно брать любые функции от регрессоров, в тесте Уайта - регрессоры, их квадраты и кросс-произведения. Тест Бройша-Пагана является асимптотическим. \\[ \\begin{cases} H_0: \\sigma^2_i = \\sigma^2 \\\\ H_1: \\sigma^2_i \\propto f(\\alpha_0 + \\alpha_1 \\cdot Z_1 + \\ldots + \\alpha_p \\cdot Z_p) \\\\ \\end{cases} \\] Классическая версия Бройша - Пагана строится на основе метода максимального правдоподобия. Предпосылками классической версии теста являются нормальность остатков, существование у функции дисперсии из альтернативной гипотезы первой и второй производной. Считается LM-статистика, которая, при верной основной гипотезе об отсутствии гетероскедастичности, имеет хи-квадратное распределение с p-1 степенью свободы. Современная модификация теста не требует нормальности остатков, лишь \\[{\\mathbb E}({\\varepsilon^4_i}) = const\\]. На первом шаге строится исходная регрессия и сохраняются остатки. Затем строится состоятельная оценка дисперсии: \\[ \\hat{\\sigma}^2 = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} {e^2_i} \\] Потом строится вспомогательная регрессия: \\[ \\frac{e^2}{\\hat{\\sigma}^2} = \\alpha_0 + \\alpha_1 \\cdot Z_1 + \\ldots + \\alpha_p \\cdot Z_p + u \\] И рассчитывается тестовая статистика: \\[ \\frac{RSS_{aux}}{2} \\sim \\chi^2_{p} \\] Тест Голдфелда - Квандта \\[ \\begin{cases} H_0: \\sigma^2_i = \\sigma^2 \\\\ H_1: \\sigma^2_i \\propto X_i \\\\ \\end{cases} \\] Этот тест предполагает нормальность остатков и является неасимптотическим. Процедура: Сначала все наблюдения сортируются по возрастанию абсолютного значения фактора, вызывающего гетероскедастичность. Затем отсортированный ряд по фактору делится на 3 примерно равные части. Считаются гетероскедастичности по первой и третьей части ряда. Строится F- статистика: \\[ \\frac{RSS_2}{RSS_1} \\sim F_{r - k, r-k} \\] WLS как способ борьбы с гетероскедастичностью Веса – оценка обратной дисперсии переменной, вызывающей гетероскедачность. То есть оценим регрессию: \\[ \\frac{\\ln{(pricemetr_i)}}{\\hat{\\sigma}_i} = \\frac{\\beta_0}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(kitsp)}} \\cdot \\ln{(kitsp_i)}}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(livesp)}} \\cdot \\ln{(livesp_i)}}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(dist)}} \\cdot \\ln{(dist_i)}}{\\hat{\\sigma}_i} + \\frac{\\beta_{\\ln{(metrdist)}} \\cdot \\ln{(metrdist_i)}}{\\hat{\\sigma}_i} + \\frac{\\varepsilon_i}{\\hat{\\sigma}_i} \\] где r - размер первой и третьей частей отсортированного ряда. "]
]
